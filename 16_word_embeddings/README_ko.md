# 수익 호출 및 SEC 제출을 위한 단어 임베딩

이 장에서는 신경망을 사용하여 단어나 단락과 같은 개별 의미 단위의 벡터 표현을 학습하는 방법을 소개합니다. 이러한 벡터는 BW(bag-of-words) 모델에서처럼 희박하지 않고 밀도가 높으며, 수만 개의 이진 또는 이산 항목이 아닌 수백 개의 실수 값을 갖습니다. 이는 각 의미 단위를 연속 벡터 공간의 위치에 할당하기 때문에 임베딩이라고 합니다.
 
임베딩은 유사한 사용법이 유사한 벡터를 의미한다는 이점을 통해 토큰을 해당 컨텍스트에 연결하도록 모델을 훈련한 결과입니다. 결과적으로 임베딩은 상대 위치를 통해 단어 간의 관계와 같은 의미론적 측면을 인코딩합니다. 이는 다음 장에서 소개할 딥 러닝 모델에 사용할 수 있는 강력한 기능입니다.

## 콘텐츠

1. __자리표시자_0__
    * __자리표시자_1__
    * __자리표시자_2__
    * __자리표시자_3__
2. __자리표시자_4__
    * __자리표시자_5__
    * __자리표시자_6__
3. __자리표시자_7__
    * __자리표시자_8__
    * __자리표시자_9__
    * __자리표시자_10__
4. __자리표시자_11__
    * __자리표시자_12__
    * __자리표시자_13__
5. __자리표시자_14__
6. __자리표시자_15__
    * __자리표시자_16__
    * __자리표시자_17__
    * __자리표시자_18__
7. __자리표시자_19__

## Word Embedding이 의미론을 인코딩하는 방법

단어 임베딩은 토큰을 저차원 벡터로 나타내므로 토큰의 상대 위치는 컨텍스트에서 사용되는 방식의 관계를 반영합니다. 그들은 단어가 속한 회사에 의해 가장 잘 정의된다는 언어학의 분포 가설을 구현합니다.

단어 벡터는 수많은 의미론적 측면을 포착할 수 있습니다. 동의어는 서로 가까울 뿐만 아니라 단어는 여러 정도의 유사성을 가질 수 있습니다. '운전자'라는 단어는 '자동차 운전자' 또는 '요인'과 유사할 수 있습니다. 또한 임베딩은 비유와 같은 단어 쌍 간의 관계를 반영합니다(일본에 대한 도쿄는 프랑스에 대한 파리, 갔다는 것은 가는 것, 본 것은 보는 것).

### 신경 언어 모델이 상황에 따라 사용법을 학습하는 방법

단어 임베딩은 주어진 문맥에 따라 단어를 예측하기 위해 얕은 신경망을 훈련한 결과입니다. 기존 언어 모델은 컨텍스트를 대상 앞에 오는 단어로 정의하는 반면, 단어 임베딩 모델은 대상을 둘러싼 대칭 창에 포함된 단어를 사용합니다.

대조적으로, bag-of-words 모델은 전체 문서를 컨텍스트로 사용하고 (가중치) 개수를 사용하여 예측 벡터가 아닌 단어의 동시 발생을 캡처합니다.

### word2vec 모델: 확장 가능한 단어 및 구문 임베딩

word2vec 모델은 텍스트 코퍼스를 입력으로 사용하고 해당 코퍼스의 단어에 대한 임베딩 벡터 세트를 출력하는 2층 신경망입니다. 얕은 신경망을 사용하여 단어 벡터를 효율적으로 학습하기 위한 두 가지 아키텍처가 있습니다.
- **Continuous-Bag-of-Words**(CBOW) 모델은 순서가 중요하지 않도록 문맥 단어 벡터의 평균을 입력으로 사용하여 대상 단어를 예측합니다. CBOW는 더 빠르게 훈련하고 자주 사용되는 용어에 대해 약간 더 정확한 경향이 있지만 자주 사용되지 않는 단어에는 덜 주의를 기울입니다.
- 이와 대조적으로 **skip-gram**(SG) 모델은 대상 단어를 사용하여 문맥에서 샘플링된 단어를 예측합니다. 소규모 데이터 세트와 잘 작동하며 희귀한 단어나 문구에 대해서도 좋은 표현을 찾습니다.

### 임베딩 평가: 벡터 산술 및 유추

단어 및 구문 벡터의 차원은 명시적인 의미를 갖지 않습니다. 그러나 임베딩은 의미론적 관계로 이어지는 방식으로 잠재 공간의 근접성과 유사한 사용법을 인코딩합니다. 이는 단어 벡터를 더하고 빼서 유추를 표현할 수 있다는 흥미로운 특성을 가져옵니다.

단어가 다른 문맥에서 사용될 수 있는 것처럼 다른 단어와도 다른 방식으로 관련될 수 있으며 이러한 관계는 잠재 공간의 다른 방향에 해당합니다. 따라서 훈련 데이터가 허용하는 경우 임베딩이 반영해야 하는 여러 유형의 비유가 있습니다.

word2vec 작성자는 임베딩 벡터의 품질을 평가하기 위해 지리, 문법 및 구문 측면과 가족 관계에 걸친 수천 개의 관계 목록을 제공합니다(디렉토리 [비유](data/analogies) 참조).

## 코드 예: 임베딩 모델 작업

다른 비지도 학습 기술과 마찬가지로 임베딩 벡터를 학습하는 목적은 텍스트 분류 또는 감정 분석과 같은 다른 작업을 위한 기능을 생성하는 것입니다.
특정 문서 모음에 대한 임베딩 벡터를 얻는 데는 여러 가지 옵션이 있습니다.
- Wikipedia 또는 Google News와 같은 일반적인 대규모 자료에서 학습된 임베딩을 사용합니다.
- 관심 영역을 반영하는 문서를 사용하여 자신만의 모델을 훈련시킵니다.

### 단어 표현을 위한 전역 벡터 작업(GloVe)

GloVe는 스탠포드 NLP 연구소에서 개발된 비지도 알고리즘으로, 집계된 전역 단어-단어 동시 발생 통계에서 단어에 대한 벡터 표현을 학습합니다(참조 참조). 다음 웹 규모 소스에서 사전 훈련된 벡터를 사용할 수 있습니다.
- 42B 또는 840B 토큰과 190만 또는 220만 토큰의 어휘를 사용한 공통 크롤링
- Wikipedia 2014 + 60억 토큰과 400,000개 토큰의 어휘가 포함된 Gigaword 5
- 20억 개의 트윗, 270억 개의 토큰 및 120만 개의 토큰 어휘를 사용하는 트위터

다음 표는 Wikipedia에서 훈련된 GloVE 벡터로 달성한 word2vec 의미론 테스트의 정확성을 보여줍니다.

| 카테고리 | 샘플 | 정확도 | 카테고리 | 샘플 | 정확도 |
|-------------|---------|----------|- --------|---------|----------|
| 자본 공유 국가 | 506 | 94.86% | 비교 | 1332 | 88.21% |
| 자본세계 | 8372 | 96.46% | 최상급 | 1056 | 74.62% |
| 시주 | 4242 | 60.00% | 현재 분사 | 1056 | 69.98% |
| 통화 | 752 | 17.42% | 국적형용사 | 1640 | 92.50% |
| 가족 | 506 | 88.14% | 과거형 | 1560 | 61.15% |
| 형용사-부사 | 992 | 22.58% | 복수 | 1332 | 78.08% |
| 반대편 | 756 | 28.57% | 복수동사 | 870 | 58.51% |

사전 훈련된 단어 임베딩을 위한 여러 소스가 있습니다. 인기 있는 옵션으로는 Stanford의 GloVE와 spaCy의 내장 벡터가 있습니다.
- [using_trained_Vectors](01_using_trained_vectors.ipynb) 노트북은 사전 학습된 벡터를 사용하여 작업하는 방법을 보여줍니다.

### 유추를 사용하여 임베딩 평가

[평가_임베딩](02_evaluating_embeddings.ipynb) 노트북은 단어 간의 유추 및 기타 의미론적 관계를 사용하여 단어 벡터의 품질을 테스트하는 방법을 보여줍니다.

## 코드 예: 금융 뉴스를 사용한 도메인별 임베딩 학습

많은 작업에는 일반 코퍼스에서 사전 훈련된 모델이 캡처할 수 없는 도메인별 어휘를 임베딩해야 합니다. 표준 word2vec 모델은 어휘에 없는 단어에 벡터를 할당할 수 없으며 대신 예측 값을 줄이는 기본 벡터를 사용합니다.

예를 들어, 산업별 문서로 작업할 때 새로운 기술이나 제품이 등장함에 따라 어휘나 사용법이 시간이 지남에 따라 변경될 수 있습니다. 결과적으로 임베딩도 발전해야 합니다. 또한 기업 수익 발표와 같은 문서에서는 Wikipedia 기사에서 사전 훈련된 GloVe 벡터가 제대로 반영될 가능성이 없는 미묘한 언어를 사용합니다.

금융 뉴스 데이터세트 소싱에 대한 지침은 [데이터](../data) 디렉터리를 참조하세요.

### 금융 뉴스 전처리: 문장 감지 및 n-gram

[금융_뉴스_전처리](03_financial_news_preprocessing.ipynb) 노트북은 모델의 소스 데이터를 준비하는 방법을 보여줍니다.

### TensorFlow 2의 스킵 그램 아키텍처 및 TensorBoard를 사용한 시각화

[Financial_news_word2vec_tensorflow](04_financial_news_word2vec_tensorflow.ipynb) 노트북은 TensorFlow 2의 Keras 인터페이스를 사용하여 word2vec 모델을 구축하는 방법을 보여줍니다. 이에 대해서는 다음 장에서 훨씬 더 자세히 소개하겠습니다.

### Gensim을 사용하여 임베딩을 더 빠르게 훈련하는 방법

TensorFlow 구현은 아키텍처 측면에서 매우 투명하지만 특별히 빠르지는 않습니다. 지난 장에서 주제 모델링에도 사용한 자연어 처리(NLP) 라이브러리 [국가로서](https://radimrehurek.com/gensim/)는 더 나은 성능을 제공하고 원저자가 제공한 C 기반 word2vec 구현과 더 유사합니다.

[Financial_news_word2vec_gensim](05_financial_news_word2vec_gensim.ipynb) 노트북은 단어 벡터를 보다 효율적으로 학습하는 방법을 보여줍니다.

## 코드 예: gensim을 사용한 SEC Filings의 단어 벡터

이 섹션에서는 알고리즘 거래를 위한 단어 임베딩의 잠재적 가치를 설명하기 위해 gensim을 사용하여 연간 SEC 제출에서 단어 및 구문 벡터를 학습합니다. 다음 섹션에서는 이러한 벡터를 가격 수익률과 기능으로 결합하여 신경망을 훈련시켜 증권 서류 내용에서 주식 가격을 예측할 것입니다.

특히 우리는 상장 기업이 제출한 2013~2016년 기간의 22,000개 이상의 10-K 연간 보고서가 포함된 데이터 세트를 사용하며 재무 정보와 경영 논평을 모두 포함합니다([대체 데이터](../03_alternative_data)의 3장 참조). 예측 모델링을 위해 데이터에 레이블을 지정할 주가가 있는 회사에 대한 11,000개의 서류 중 약 절반에 해당합니다(자세한 내용은 [초 신고](sec-filings) 폴더의 데이터 소스 및 노트북에 대한 참조 참조).

- __자리표시자_30__
- __자리표시자_31__

### 전처리: 콘텐츠 선택, 문장 감지, n-그램

[sec_전처리](06_sec_preprocessing.ipynb) 노트북은 14장 [거래를 위한 텍스트 데이터: 감정 분석](../14_working_with_text_data)의 접근 방식과 유사하게 spaCy를 사용하여 텍스트를 구문 분석하고 토큰화하는 방법을 보여줍니다.

### 모델 훈련 및 평가

노트북 [sec_word2vec](07_sec_word2vec.ipynb)는 gensim의 스킵 그램 아키텍처 구현을 사용하여 SEC 서류 데이터 세트에 대한 단어 벡터를 학습합니다.

## 코드 예: Doc2Vec을 사용한 감정 분석

텍스트 분류에는 여러 단어 임베딩을 결합해야 합니다. 일반적인 접근 방식은 문서의 각 단어에 대한 임베딩 벡터의 평균을 구하는 것입니다. 이는 모든 임베딩의 정보를 사용하고 벡터 추가를 효과적으로 사용하여 임베딩 공간의 다른 위치 지점에 도달합니다. 그러나 단어 순서에 대한 관련 정보는 손실됩니다.

대조적으로, 단락이나 제품 리뷰와 같은 텍스트 조각에 대한 최첨단 임베딩 생성은 문서 임베딩 모델 doc2vec을 사용하는 것입니다. 이 모델은 word2vec 작성자가 원본 기여를 게시한 직후 개발되었습니다. word2vec과 유사하게 doc2vec에도 두 가지 종류가 있습니다.
- DBOW(Distributed Bag of Words) 모델은 Word2Vec CBOW 모델에 해당합니다. 문서 벡터는 문맥 단어 벡터와 문서의 문서 벡터를 모두 기반으로 대상 단어를 예측하는 합성 작업에 대해 네트워크를 훈련한 결과입니다.
- 분산 메모리(DM) 모델은 word2wec 스킵그램 아키텍처에 해당합니다. 문서 벡터는 전체 문서의 문서 벡터를 사용하여 대상 단어를 예측하도록 신경망을 훈련한 결과입니다.

[doc2vec_yelp_sentiment](08_doc2vec_yelp_sentiment.ipynb) 노트북은 관련 별점과 함께 100만 개의 Yelp 리뷰의 무작위 샘플에 doc2vec를 적용합니다.

## 새로운 영역: 주의, 변환기 및 사전 훈련

Word2vec 및 GloVe 임베딩은 단어 백 접근 방식보다 더 많은 의미 정보를 캡처하지만 상황별 사용법을 구별하지 않는 각 토큰의 단일 고정 길이 표현만 허용합니다. 다의어라고 불리는 동일한 단어에 대한 다중 의미와 같은 해결되지 않은 문제를 해결하기 위해 보다 상황에 맞는 단어 임베딩([Fassounet al. 2017년](https://arxiv.org/abs/1706.03762))을 학습하도록 설계된 주의 메커니즘을 기반으로 하는 몇 가지 새로운 모델이 등장했습니다. 이 모델의 주요 특징은 다음과 같습니다. 
- 보다 풍부한 컨텍스트 표현을 위해 텍스트를 왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로 처리하는 양방향 언어 모델을 사용합니다.
- 특정 작업에 맞게 최종적으로 미세 조정되는 임베딩 및 네트워크 가중치 형태로 범용 언어 측면을 학습하기 위해 대규모 일반 코퍼스에 대한 반지도 사전 학습 사용

### 주의가 필요한 전부: 자연어 생성을 변화시키다

2018년에 Google은 Transformers의 양방향 인코더 표현([Devlinet al. 2019](https://arxiv.org/abs/1810.04805))을 의미하는 BERT 모델을 출시했습니다. NLP 연구의 획기적인 발전으로, 일반 언어 이해 평가(GLUE) [기준](https://gluebenchmark.com/)로 측정한 질문 답변 및 명명된 엔터티 인식부터 패러프레이징 및 감정 분석에 이르는 11가지 자연어 이해 작업에서 획기적인 결과를 달성했습니다.

- __자리표시자_40__
- __자리표시자_41__

### BERT: 보다 보편적이고 사전 훈련된 언어 모델을 향하여

BERT 모델은 두 가지 핵심 아이디어, 즉 이전 섹션에서 설명한 변환기 아키텍처와 감독되지 않은 사전 학습을 기반으로 구축되므로 새로운 작업마다 처음부터 학습할 필요가 없습니다. 오히려 가중치가 미세 조정됩니다.
- BERT는 아키텍처에 따라 12개 또는 16개의 주의 헤드가 있는 12개 또는 24개의 레이어를 사용하여 주의 메커니즘을 새로운(더 깊은) 수준으로 끌어올려 상황별 임베딩을 학습하기 위한 최대 24 x 16 = 384개의 주의 메커니즘을 생성합니다.  
- BERT는 감독되지 않은 양방향 사전 훈련을 사용하여 마스크된 언어 모델링(왼쪽 및 오른쪽 컨텍스트에서 누락된 단어 예측)과 다음 문장 예측(한 문장이 다른 문장 뒤에 오는지 예측)이라는 두 가지 작업에 대한 가중치를 미리 학습합니다.

- __자리표시자_42__
- __자리표시자_43__
- __자리표시자_44__

### 사전 훈련된 최첨단 모델 사용

- __자리표시자_45__
    - Transformers(이전의 pytorch-transformers 및 pytorch-pretrained-bert)는 최첨단 범용 아키텍처(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL...)를 제공합니다. 100개 이상의 언어로 사전 학습된 수천 개 이상의 모델과 PyTorch와 TensorFlow 2.0 간의 심층적인 상호 운용성을 갖춘 자연어 이해(NLU) 및 자연어 생성(NLG)을 위한 것입니다.
- __자리표시자_46__
    - 이 패키지(이전 spacy-pytorch-transformers)는 Hugging Face의 변환기 패키지를 래핑하는 spaCy 모델 파이프라인을 제공하므로 spaCy에서 사용할 수 있습니다. 그 결과 BERT, GPT-2, XLNet 등과 같은 최첨단 변압기 아키텍처에 편리하게 액세스할 수 있습니다. 자세한 내용과 배경 정보는 다음과 같습니다.
- __자리표시자_47__
    - NLP를 위한 딥 러닝: AllenNLP를 사용하면 클라우드나 노트북에서 쉽게 실행할 수 있는 인프라와 함께 거의 모든 NLP 문제에 대한 새로운 딥 러닝 모델을 쉽게 설계하고 평가할 수 있습니다.
    - 최첨단 모델: AllenNLP에는 핵심 NLP 문제(예: 의미론적 역할 라벨링) 및 NLP 애플리케이션(예: 텍스트 수반)에 대한 고품질 모델의 참조 구현이 포함되어 있습니다.
- [문장 변환기: BERT / RoBERTa / XLM-RoBERTa & Co. with PyTorch를 사용한 다국어 문장 임베딩]
     -BERT / RoBERTa / XLM-RoBERTa는 기본적으로 다소 나쁜 문장 임베딩을 생성합니다. 이 저장소는 샴 또는 삼중 네트워크 구조로 BERT / RoBERTa / DistilBERT / ALBERT / XLNet을 미세 조정하여 비지도 시나리오에서 사용할 수 있는 의미상 의미 있는 문장 임베딩을 생성합니다. 코사인 유사성, 클러스터링, 의미 검색을 통한 의미 텍스트 유사성.

## 추가 리소스

- __자리표시자_48__
- __자리표시자_49__
- __자리표시자_50__
- __자리표시자_51__
- __자리표시자_52__
- __자리표시자_53__
- __자리표시자_54__
- __자리표시자_55__