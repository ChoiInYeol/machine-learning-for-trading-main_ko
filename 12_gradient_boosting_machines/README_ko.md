# 거래 전략 강화

이 장에서는 일반적으로 `sklearn`보다 더 나은 결과를 생성하는 의사결정 트리를 기반으로 하는 또 다른 앙상블 학습 알고리즘인 부스팅을 살펴봅니다.

주요 차이점은 원래 AdaBoost 버전의 부스팅은 새 트리를 추가하기 전에 모델에서 발생한 누적 오류를 기반으로 각 트리의 교육 데이터를 수정한다는 것입니다. 이와 대조적으로 Random Forest는 배깅을 사용하여 훈련 세트의 무작위 버전을 사용하여 서로 독립적으로 많은 나무를 훈련합니다. 랜덤 포레스트는 병렬로 학습될 수 있지만, 부스팅은 가중치가 다시 부여된 데이터 버전을 사용하여 순차적으로 진행됩니다. 최첨단 부스팅 구현은 또한 랜덤 포레스트의 무작위화 전략을 채택합니다.

이 장에서는 지난 30년 동안 부스팅이 어떻게 가장 성공적인 기계 학습 알고리즘 중 하나로 발전했는지 살펴보겠습니다. 글을 쓰는 시점에는 구조화된 데이터에 대한 기계 학습 경쟁을 지배하게 되었습니다(예를 들어 입력과 출력 간의 관계가 더 복잡하고 딥 러닝이 뛰어난 고차원 이미지나 음성과 반대). 보다 구체적으로, 이 장에서는 다음 주제를 다룰 것입니다.

## 콘텐츠

1. __자리표시자_1__
    * __자리표시자_2__
    * __자리표시자_3__
2. __자리표시자_4__
    * __자리표시자_5__
    * __자리표시자_6__
3. __자리표시자_7__
4. __자리표시자_8__
    * __자리표시자_9__
    * __자리표시자_10__
    * __자리표시자_11__
    * __자리표시자_12__
    * __자리표시자_13__
5. __자리표시자_14__
    * __자리표시자_15__
        - __자리 표시자_16__
        - __자리표시자_17__
        - __자리표시자_18__
6. __자리표시자_19__
    * __자리표시자_20__
    * __자리표시자_21__
7. __자리표시자_22__
    * __자리표시자_23__
    * __자리표시자_24__
    * __자리표시자_25__

## 시작하기: 적응형 부스팅

배깅과 마찬가지로 부스팅은 기본 학습자를 앙상블로 결합합니다. 부스팅은 처음에는 분류 문제를 위해 개발되었지만 회귀에도 사용될 수 있으며 지난 20년 동안 소개된 가장 강력한 학습 아이디어 중 하나로 불려 왔습니다(Trevor Hastie 등의 [통계적 학습의 요소](http://web.stanford.edu/~hastie/ElemStatLearn/)에 설명됨). 배깅과 마찬가지로 많은 통계 학습 모델에 적용할 수 있는 일반적인 방법 또는 메타 방법입니다.

다음 섹션에서는 AdaBoost를 간략하게 소개한 다음 그래디언트 부스팅 모델과 이 알고리즘의 여러 최첨단 구현에 중점을 둡니다.

### AdaBoost 알고리즘

AdaBoost는 편향을 줄이기 위해 매우 깊은 나무에 앙상블을 구축하는 배깅에서 크게 벗어났습니다. 이와 대조적으로 AdaBoost는 약한 학습기로 얕은 나무를 키우며 종종 그루터기, 즉 단일 분할로 형성된 나무로 뛰어난 정확도를 생성합니다. 알고리즘은 동일한 가중치의 훈련 세트로 시작한 다음 샘플 분포를 연속적으로 변경합니다. 각 반복 후에 AdaBoost는 잘못 분류된 관찰의 가중치를 늘리고 올바르게 예측된 샘플의 가중치를 줄여 후속 약한 학습기가 특히 어려운 사례에 더 집중할 수 있도록 합니다. 훈련이 완료되면 새로운 의사결정 트리는 훈련 오류 감소에 대한 기여도를 반영하는 가중치를 사용하여 앙상블에 통합됩니다.

- [온라인 학습의 의사결정이론적 일반화와 부스팅 적용](http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf), Y. Freund, R. Schapire, 1997.

### 코드 예: sklearn을 사용한 AdaBoost

앙상블 모듈의 일부로 sklearn은 두 개 이상의 클래스를 지원하는 [AdaBoost 분류기](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) 구현을 제공합니다.

이 섹션의 코드 예제는 다양한 알고리즘의 성능을 항상 가장 빈번한 클래스를 예측하는 더미 분류기와 비교하는 노트북 [gbm_baseline](01_gbm_baseline.ipynb)에 있습니다.

이 장의 알고리즘은 먼저 실행되어야 하는 [알파인자 연구에 관한 제4장](../04_alpha_factor_research)의 노트북 [기능 엔지니어링](../04_alpha_factor_research/00_data/feature_engineering.ipynb)에 의해 생성된 데이터 세트를 사용합니다.

- [랜덤 포레스트](../10_decision_trees_random_forests) AdaBoost [문서](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)

## 그라데이션 부스팅 - 대부분의 작업에 대한 앙상블

GBM(Gradient Boosting Machines) 알고리즘의 기본 아이디어는 앙상블의 현재 손실 함수에 대한 음의 기울기를 학습하도록 기본 학습기를 훈련하는 것입니다. 결과적으로, 앙상블에 추가할 때마다 이전 앙상블 구성원이 저지른 오류를 고려하여 전반적인 훈련 오류를 줄이는 데 직접적으로 기여합니다. 각각의 새로운 멤버는 데이터의 새로운 함수를 나타내기 때문에 그래디언트 부스팅은 함수 hm을 추가 방식으로 최적화한다고도 합니다.

- [그리디 함수 근사: 그래디언트 부스팅 머신](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf), 제롬 H. 프리드먼, 1999

### GBM 모델을 훈련하고 조정하는 방법

그래디언트 부스팅 성능의 두 가지 주요 동인은 앙상블의 크기와 구성 결정 트리의 복잡성입니다. 의사결정 트리의 복잡성 제어는 일반적으로 리프 노드의 매우 적은 수의 샘플을 암시하는 매우 구체적인 규칙을 학습하는 것을 방지하는 것을 목표로 합니다. [4장 의사결정 트리와 랜덤 포레스트](../10_decision_trees_random_forests)의 훈련 데이터에 과적합되는 의사결정 트리의 기능을 제한하는 데 사용되는 가장 효과적인 제약 조건을 다루었습니다.

앙상블의 크기를 직접 제어하는 ​​것 외에도 [7장, 선형 모델 - 회귀 및 분류](../07_linear_models)의 Ridge 및 Lasso 선형 회귀 모델의 맥락에서 접한 축소와 같은 다양한 정규화 기술이 있습니다. 또한, 랜덤 포레스트의 맥락에서 사용되는 무작위화 기술은 그라디언트 부스팅 머신에도 일반적으로 적용됩니다.

### 코드 예: scikit-learn을 사용한 경사 부스팅

sklearn의 앙상블 모듈에는 바이너리 및 멀티클래스 모두에서 회귀 및 분류를 위한 그래디언트 부스팅 트리 구현이 포함되어 있습니다.

[부스팅_기준선](./01_boosting_baseline.ipynb) 노트북은 [그래디언트부스팅 분류기](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)에 대한 교차 검증을 실행하는 방법을 보여줍니다.

[sklearn_gbm_tuning](02_sklearn_gbm_tuning.ipynb) 노트북은 [GridSearchCV]()를 사용하여 최상의 매개변수 세트를 검색하는 방법을 보여줍니다. 실행하는 데 시간이 많이 걸릴 수 있습니다.

[sklearn_gbm_tuning_results](03_sklearn_gbm_tuning_results.ipynb) 노트북에는 얻을 수 있는 일부 결과가 표시됩니다.

- `scikit-klearn` 그래디언트 부스팅 [문서](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)

## XGBoost, LightGBM 및 CatBoost 사용

지난 몇 년 동안 여러 가지 새로운 그래디언트 부스팅 구현에서는 훈련을 가속화하고 리소스 효율성을 향상시키며 알고리즘을 매우 큰 데이터 세트로 확장할 수 있는 다양한 혁신을 사용했습니다. 새로운 구현과 해당 소스는 다음과 같습니다.
- [XGBoost](https://github.com/dmlc/xgboost)(극단적 그래디언트 부스팅), 2014년 워싱턴 대학의 Tianqi Chen이 시작함 
- [라이트GBM](https://github.com/Microsoft/LightGBM)(Microsoft에서 2017년 1월에 처음 출시함)
- [캣부스트](https://tech.yandex.com/catboost/), Yandex에서 2017년 4월에 처음 출시됨

이 책에서는 구현을 설명하기 전에 시간이 지남에 따라 등장하고 이후에 수렴(대부분의 기능을 모든 구현에 사용할 수 있도록)한 수많은 알고리즘 혁신을 검토합니다.



## 코드 예: 그래디언트 부스팅을 사용한 롱숏 거래 전략

이 섹션에서는 그래디언트 부스팅 모델로 생성된 일일 수익률 예측을 바탕으로 미국 주식에 대한 거래 전략을 설계, 구현 및 평가합니다.

이전 예제에서와 마찬가지로 프레임워크를 배치하고 자체 실험을 실행하기 위해 조정할 수 있는 특정 예제를 구축하겠습니다. 자산 클래스 및 투자 영역부터 기능, 보유 기간 또는 거래 규칙과 같은 보다 세부적인 측면에 이르기까지 다양할 수 있는 측면이 있습니다. 예를 들어 다양한 추가 기능을 보려면 부록의 Alpha Factor Library를 참조하세요.

### 데이터 준비

우리는 Quandl Wiki 데이터를 사용하여 몇 가지 간단한 기능을 설계하고(자세한 내용은 노트북 [준비_the_model_data](04_preparing_the_model_data.ipynb) 참조) 2015/16년을 검증 기간으로 사용하는 모델을 선택하고 2017년에 대한 샘플 외 테스트를 실행합니다.

### LightGBM 및 CatBoost 모델로 신호를 생성하는 방법

우리는 거래 전략을 단순하게 유지하고 단일 기계 학습(ML) 신호만 사용할 것입니다. 실제 애플리케이션은 서로 다른 데이터 세트 또는 서로 다른 예측 또는 전환 기간으로 훈련된 보완적인 ML 모델과 같은 다양한 소스의 여러 신호를 사용할 가능성이 높습니다. 또한 단순한 손절매부터 위험 가치 분석까지 정교한 위험 관리를 사용합니다.

XGBoost, LightGBM 및 CatBoost는 Python을 포함한 여러 언어에 대한 인터페이스를 제공하며 GridSearchCV와 같은 다른 sklearn 기능과 호환되는 sklearn 인터페이스와 그래디언트 부스팅 모델을 훈련하고 예측하는 자체 방법을 모두 갖추고 있습니다. 노트북 [gbm_baseline](01_gbm_baseline.ipynb)은 각 구현에 대한 sklearn 인터페이스 사용을 보여줍니다. 라이브러리 메서드는 종종 더 잘 문서화되어 있고 사용하기도 쉽기 때문에 이러한 모델의 사용법을 설명하는 데 이를 사용하겠습니다.

이 프로세스에는 라이브러리별 데이터 형식 생성, 다양한 하이퍼 매개변수 조정 및 다음 섹션에서 설명할 결과 평가가 수반됩니다.

- 노트북 [Trading_signals_with_lightgbm_and_catboost](05_trading_signals_with_lightgbm_and_catboost.ipynb)은 다양한 하이퍼 매개변수 옵션을 교차 검증하여 모델의 예측 성능을 최적화합니다.

### 거래 신호 평가

노트북 [평가_거래_신호](06_evaluate_trading_signals.ipynb)은 거래 신호를 평가하는 방법을 보여줍니다.

### 표본 외 예측 생성

[Making_out_of_sample_predictions](08_making_out_of_sample_predictions.ipynb) 노트북은 최고 성능 모델에 대한 예측을 생성하는 방법을 보여줍니다.

### 롱숏 전략 정의 및 백테스트

[백테스팅_with_zipline](09_backtesting_with_zipline.ipynb) 노트북은 모델 예측을 기반으로 전략을 생성하고 [지퍼 라인](https://zipline.ml4trading.io/, and evaluates the result using [Pyfolio](https://github.com/quantopian/pyfolio. 

## A peek into the black box: How to interpret GBM results

Understanding why a model predicts a certain outcome is very important for several reasons, including trust, actionability, accountability, and debugging. Insights into the nonlinear relationship between features and the outcome uncovered by the model, as well as interactions among features, are also of value when the goal is to learn more about the underlying drivers of the phenomenon under study.

### Code example: attributing feature importance with LightGBM

The notebook [model_interpretation](06_model_interpretation.ipynb)을 사용하여 과거 성능을 시뮬레이션하며 다음 방법을 보여줍니다.

#### 기능 중요도

전역 특성 중요도 값을 계산하는 세 가지 기본 방법은 다음과 같습니다.
- 이득: 1984년 Leo Breiman이 도입한 이 고전적인 접근 방식은 주어진 기능에 대한 모든 분할로 인한 손실 또는 불순물의 전체 감소를 ​​사용합니다. 동기는 대체로 경험적이지만 특징을 선택하는 데 일반적으로 사용되는 방법입니다.
- 분할 카운트: 이는 결과적인 정보 획득을 기반으로 이 목적을 위한 기능 선택을 기반으로 분할 결정을 내리는 데 기능이 사용되는 빈도를 계산하는 대체 접근 방식입니다.
- 순열: 이 접근 방식은 테스트 세트의 특성 값을 무작위로 순열하고 중요한 특성이 예측 오류를 크게 증가시켜야 한다는 가정 하에 모델의 오류가 얼마나 변경되는지 측정합니다. 다양한 순열 선택으로 인해 이 기본 접근 방식이 대안으로 구현됩니다.

#### 부분 의존도 도표

모델 예측에 대한 개별 기능의 요약 기여 외에도 부분 의존성 플롯은 대상 변수와 기능 집합 간의 관계를 시각화합니다. 그래디언트 부스팅 트리의 비선형 특성으로 인해 이 관계는 다른 모든 기능의 값에 따라 달라집니다.

#### SHApley 추가 설명(SHAP 값)

2017 NIPS 컨퍼런스에서 워싱턴 대학의 Scott Lundberg와 Su-In Lee는 [SHApley 첨가제 설명](https://github.com/slundberg/shap) 또는 SHAP 값이라는 트리 앙상블 모델의 출력에 대한 개별 기능의 기여를 설명하는 새롭고 더 정확한 접근 방식을 제시했습니다.

이 새로운 알고리즘은 이전에 살펴본 것과 같은 트리 앙상블에 대한 기능 기여 방법이 일관성이 없다는 관찰에서 출발합니다. 즉, 출력에 대한 기능의 영향을 증가시키는 모델의 변경으로 인해 중요도가 낮아질 수 있습니다. 이 기능의 값입니다.

SHAP 가치는 협력적 게임 이론과 지역적 설명의 아이디어를 통합하며 기대에 따라 이론적으로 최적이고 일관되며 지역적으로 정확한 것으로 나타났습니다. 가장 중요한 것은 Lundberg와 Lee가 O(TLDM)에서 O(TLD2)(여기서 T와 M은 트리와 기능의 수)로 모델에 구애받지 않는 추가 기능 속성 방법을 계산하는 복잡성을 줄이는 알고리즘을 개발했다는 ​​것입니다. , 각각 D와 L은 나무 전체의 최대 깊이와 나뭇잎 수입니다.

이 중요한 혁신을 통해 이전에는 다루기 어려웠던 수천 그루의 나무와 특징이 포함된 모델의 예측을 몇 분의 1초 만에 설명할 수 있게 되었습니다. 오픈 소스 구현은 2017년 후반에 제공되었으며 XGBoost, LightGBM, CatBoost 및 sklearn 트리 모델과 호환됩니다.

Shapley 가치는 팀 성공에 대한 기여도를 반영하는 협력 게임에서 각 플레이어에게 가치를 할당하는 기술인 게임 이론에서 유래되었습니다. SHAP 값은 게임 이론 개념을 트리 기반 모델에 적용한 것이며 각 기능과 각 샘플에 대해 계산됩니다. 특정 관찰에 대한 모델 출력에 기능이 어떻게 기여하는지 측정합니다. 이러한 이유로 SHAP 값은 특성의 영향이 샘플에 따라 어떻게 달라지는지에 대한 차별화된 통찰력을 제공하며, 이는 이러한 비선형 모델에서 상호 작용 효과의 역할을 고려할 때 중요합니다.

SHAP 값은 각 개별 예측 수준에서 세부적인 기능 속성을 제공하고 (대화형) 시각화를 통해 복잡한 모델을 훨씬 더 풍부하게 검사할 수 있습니다. 이 섹션의 시작 부분에 표시된 SHAP 요약 산점도는 전역 기능 중요도 막대 차트보다 훨씬 더 차별화된 통찰력을 제공합니다. 개별 클러스터 예측의 힘 플롯을 사용하면 보다 자세한 분석이 가능하고, SHAP 종속성 플롯은 상호 작용 효과를 포착하여 결과적으로 부분 의존성 플롯보다 더 정확하고 자세한 결과를 제공합니다.

## Algoseek 및 LightGBM을 사용한 일중 전략

이 섹션과 노트북은 Algoseek에서 샘플 데이터를 사용할 수 있게 되면 업데이트될 것입니다.

### 코드 예: 일중 기능 엔지니어링

__PLACEHOLDER__ 52 __ 노트북은 분 단위 거래 및 견적 데이터에서 기능을 생성합니다.

### 코드 예: LightGBM 모델 조정 및 예측 평가

[인트라데이_모델](11_intraday_model.ipynb) 노트북은 LightGBM 부스팅 모델을 최적화하고, 샘플 외부 예측을 생성하고, 결과를 평가합니다.

## 자원

- __자리표시자_54__
- __자리표시자_55__
- __자리표시자_56__
- __자리표시자_57__

### XGBoost

- __자리표시자_58__
- __자리표시자_59__
- __자리표시자_60__
- __자리표시자_61__
- __자리표시자_62__
- __자리표시자_63__
- __자리표시자_64__
- __자리표시자_65__

### 라이트GBM

- __자리표시자_66__
- __자리표시자_67__
- __자리표시자_68__
- __자리표시자_69__
- __자리표시자_70__
- __자리표시자_71__

### 캣부스트

- __자리표시자_72__
- __자리표시자_73__
- __자리표시자_74__