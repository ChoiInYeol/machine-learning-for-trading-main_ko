{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=\"+2\">더블 Deep Q-Learning 및 Open AI Gym: 소개</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Open AI 달 착륙선 환경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[오픈AI 체육관](https://gym.openai.com/)은 Python을 사용하여 RL 알고리즘을 테스트하고 벤치마킹하기 위한 표준화된 환경을 제공하는 RL 플랫폼입니다. 플랫폼 확장 및 맞춤형 환경 등록도 가능합니다.",
        "\n",
        "[달착륙선](https://gym.openai.com/envs/LunarLander-v2)(LL) 환경에서는 에이전트가 위치, 방향 및 속도를 포함하는 이산 작업 공간과 저차원 상태 관찰을 기반으로 2차원에서 동작을 제어해야 합니다. 각 단계에서 환경은 새로운 상태에 대한 관찰과 긍정적이거나 부정적인 보상을 제공합니다.  각 에피소드는 최대 1,000개의 시간 단계로 구성됩니다. 다음 다이어그램은 훈련할 에이전트가 250개의 에피소드를 수행한 후 성공적인 착륙에서 선택한 프레임을 보여줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"../assets/lunar_lander.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "보다 구체적으로 에이전트는 6개의 연속 요소와 2개의 개별 요소를 포함하여 상태의 8가지 측면을 관찰합니다. 관찰된 요소를 기반으로 에이전트는 위치, 방향, 이동 속도 및 (부분적으로) 착륙했는지 여부를 파악합니다. 그러나 사용 가능한 작업을 사용하여 어디로 움직여야 하는지 모르거나 동작을 제어하는 ​​규칙을 이해한다는 의미에서 환경의 내부 상태를 관찰하지 않습니다.",
        "\n",
        "각 시간 단계에서 에이전트는 네 가지 개별 작업 중 하나를 사용하여 모션을 제어합니다. 아무것도 할 수 없으며(현재 경로를 계속 유지) 주 엔진을 발사하거나(하향 동작을 줄이기 위해) 각 방향 엔진을 사용하여 왼쪽이나 오른쪽으로 조종할 수 없습니다. 연료 제한은 없습니다.",
        "\n",
        "목표는 좌표 (0, 0)의 랜딩 패드에 있는 두 개의 플래그 사이에 에이전트를 착륙시키는 것이지만 패드 외부에도 착륙이 가능합니다. 에이전트는 정확한 착지 지점에 따라 패드를 향해 이동하면서 100~140 범위의 보상을 축적합니다. 그러나 대상에서 멀어지면 에이전트가 패드 쪽으로 이동하여 얻을 수 있는 보상이 무효화됩니다. 각 다리의 지면 접촉은 10점을 추가하며, 주 엔진 사용 비용은 -0.3점입니다.",
        "\n",
        "에이전트가 착륙하거나 충돌하면 각각 100점을 더하거나 빼거나 1,000시간 단계 후에 에피소드가 종료됩니다. LL을 해결하려면 100회 연속 에피소드에서 평균 최소 200의 누적 보상을 달성해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 딥 Q-러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deep Q 학습은 심층 신경망을 사용하여 특정 상태에 대해 사용 가능한 작업의 값을 추정합니다. 이는 RL 에이전트가 픽셀 입력만으로 게임을 플레이하는 방법을 학습한 Deep Mind의 [심층 강화 학습으로 Atari 플레이하기](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)(2013)에 의해 소개되었습니다.",
        "\n",
        "Deep Q-Learning 알고리즘은 $$q(s,a,\\theta)\\about이 되도록 상태를 동작에 매핑하는 다층 딥 Q 네트워크(DQN)의 가중치 세트를 학습하여 동작-값 함수 q를 근사화합니다. q^*(s,a)$$",
        "\n",
        "알고리즘은 목표에 대한 DQN 추정치 간의 제곱 차이로 정의된 손실 함수에 경사하강법을 적용합니다.",
        "$$y_i=\\mathbb{E}[r+\\gamma\\max_{a^\\prime}Q(s^\\prime, a^\\prime; \\theta_{i−1}\\mid s,a)]$$",
        "네트워크 매개변수를 학습하기 위한 현재 상태-작업 쌍의 작업-값 추정:",
        "\n",
        "$$L_i(\\theta_i)=\\mathbb{E}\\left[\\left(\\overbrace{\\underbrace{y_i}_{\\text{Q 대상}}−\\underbrace{Q(s, a; \\theta)} _{\\text{현재 예측}}}^{\\text{TD 오류}}\\right)^2 \\right]$$",
        "\n",
        "목표와 현재 추정치는 모두 가중치 세트에 따라 달라지며, 이는 훈련 전에 목표가 고정되는 지도 학습과의 차이점을 강조합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 확장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "몇 가지 혁신을 통해 심층 Q-학습의 정확성과 수렴 속도가 향상되었습니다.",
        "- **경험 재생**은 상태, 행동, 보상 및 다음 상태 전환의 기록을 저장하고 에이전트가 ε-탐욕적인 행동을 선택하기 전에 각 시간 단계에서 네트워크 가중치를 업데이트하기 위해 이 경험에서 미니 배치를 무작위로 샘플링합니다. 이는 샘플 효율성을 높이고, 샘플의 자기 상관을 감소시키며, 로컬 최소값 또는 발산으로 이어질 수 있는 훈련 샘플을 생성하는 현재 가중치로 인해 피드백을 제한합니다.",
        "- **천천히 변화하는 대상 네트워크**는 신경망 가중치 업데이트에 대한 현재 네트워크 매개변수의 피드백 루프를 약화시킵니다. [심층 강화 학습을 통한 인간 수준의 제어](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)(2015)에서 Deep Mind가 개발한 이 네트워크는 Q 네트워크와 동일한 아키텍처를 갖는 천천히 변화하는 대상 네트워크를 사용하지만 가중치는 주기적으로만 업데이트됩니다. 타겟 네트워크는 현재 상태 값에 대한 Q-Networks 추정을 업데이트하는 데 사용되는 다음 상태 값에 대한 예측을 생성합니다.",
        "- **이중 딥 Q-러닝**은 의도적으로 가장 높은 액션 값을 샘플링하기 때문에 액션 값을 과대평가하는 딥 Q-러닝의 편향을 해결합니다. Hado van Hasselt가 [Double Q-learning을 이용한 심층 강화 학습](https://arxiv.org/abs/1509.06461)(2015)에서 보여준 것처럼 이러한 편견은 균일하게 적용되지 않으면 학습 과정과 결과 정책에 부정적인 영향을 미칠 수 있습니다. 작업 선택에서 작업 값 추정을 분리하기 위해 DDQN(Double Deep Q-Learning)은 한 네트워크의 가중치를 사용하여 다음 상태에서 최상의 작업을 선택하고 다른 네트워크의 가중치를 사용하여 해당 작업 값을 제공합니다. 추정."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 가져오기 및 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 아래 코드에 필요한 TensorFlow를 버전 2.2로 업그레이드하는 방법에 대한 지침은 노트북 `04_q_learning_for_trading.ipynb`을 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:14.434999Z",
          "start_time": "2021-02-25T05:54:14.432713Z"
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.793029Z",
          "start_time": "2021-02-25T05:54:14.436730Z"
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# OpenAI Gym\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.795788Z",
          "start_time": "2021-02-25T05:54:15.793935Z"
        }
      },
      "outputs": [],
      "source": [
        "sns.set_style('whitegrid', {'axes.grid' : False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.823604Z",
          "start_time": "2021-02-25T05:54:15.796738Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    print('Using GPU')\n",
        "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "else:\n",
        "    print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "결과를 재현할 수 있도록 무작위 시드를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.837459Z",
          "start_time": "2021-02-25T05:54:15.824621Z"
        }
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결과 표시 도우미 기능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.845786Z",
          "start_time": "2021-02-25T05:54:15.838373Z"
        }
      },
      "outputs": [],
      "source": [
        "def format_time(t):\n",
        "    m_, s = divmod(t, 60)\n",
        "    h, m = divmod(m_, 60)\n",
        "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Docker 컨테이너에서 실행되도록 가상 디스플레이 활성화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이는 디스플레이가 없는 서버에서 실행하는 경우에만 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.856704Z",
          "start_time": "2021-02-25T05:54:15.846670Z"
        }
      },
      "outputs": [],
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# virtual_display = Display(visible=0, size=(1400, 900))\n",
        "# virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDQN 에이전트 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[텐서플로우](https://www.tensorflow.org/)을(를) 사용하여 Double Deep Q-Network를 생성하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 재생 버퍼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.864953Z",
          "start_time": "2021-02-25T05:54:15.857578Z"
        }
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, capacity, state_dims):\n",
        "        self.capacity = capacity\n",
        "        self.idx = 0\n",
        "\n",
        "        self.state_memory = np.zeros(shape=(capacity, state_dims), \n",
        "                                     dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros_like(self.state_memory)\n",
        "\n",
        "        self.action_memory = np.zeros(capacity, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros_like(self.action_memory)\n",
        "        self.done = np.zeros_like(self.action_memory)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        self.state_memory[self.idx, :] = state\n",
        "        self.new_state_memory[self.idx, :] = next_state\n",
        "        self.reward_memory[self.idx] = reward\n",
        "        self.action_memory[self.idx] = action\n",
        "        self.done[self.idx] = 1 - int(done)\n",
        "        self.idx += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = np.random.choice(self.idx, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        next_states = self.new_state_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        done = self.done[batch]\n",
        "        return states, actions, rewards, next_states, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 에이전트 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.879586Z",
          "start_time": "2021-02-25T05:54:15.866405Z"
        }
      },
      "outputs": [],
      "source": [
        "class DDQNAgent:\n",
        "    def __init__(self,\n",
        "                 state_dim,\n",
        "                 num_actions,\n",
        "                 gamma,\n",
        "                 epsilon_start,\n",
        "                 epsilon_end,\n",
        "                 epsilon_decay_steps,\n",
        "                 epsilon_exponential_decay,\n",
        "                 learning_rate,\n",
        "                 architecture,\n",
        "                 l2_reg,\n",
        "                 replay_capacity,\n",
        "                 tau,\n",
        "                 batch_size,\n",
        "                 results_dir,\n",
        "                 log_every=10):\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.architecture = architecture\n",
        "        self.l2_reg = l2_reg\n",
        "        self.learning_rate = learning_rate\n",
        "        self.experience = Memory(replay_capacity,\n",
        "                                 state_dim)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.idx = np.arange(batch_size, dtype=np.int32)\n",
        "\n",
        "        self.online_network = self.build_model()\n",
        "        self.target_network = self.build_model(trainable=False)\n",
        "        self.optimizer = Adam(lr=learning_rate)\n",
        "        self.update_target()\n",
        "\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
        "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
        "        self.epsilon_history = []\n",
        "\n",
        "        self.total_steps = self.train_steps = 0\n",
        "        self.episodes = self.episode_length = self.train_episodes = 0\n",
        "        self.steps_per_episode = []\n",
        "        self.episode_reward = 0\n",
        "        self.rewards_history = []\n",
        "\n",
        "        self.results_dir = results_dir\n",
        "        self.experiment = experiment\n",
        "        self.log_every = log_every\n",
        "        \n",
        "        self.summary_writer = (tf.summary\n",
        "                               .create_file_writer(results_dir.as_posix()))\n",
        "        self.start = time()\n",
        "        self.train = True\n",
        "\n",
        "    def build_model(self, trainable=True):\n",
        "        layers = []\n",
        "        for i, units in enumerate(self.architecture, 1):\n",
        "            layers.append(Dense(units=units,\n",
        "                                input_dim=self.state_dim if i == 1 else None,\n",
        "                                activation='relu',\n",
        "                                kernel_regularizer=l2(self.l2_reg),\n",
        "                                trainable=trainable))\n",
        "        layers.append(Dense(units=self.num_actions,\n",
        "                            trainable=trainable))\n",
        "        return Sequential(layers)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_network.set_weights(self.online_network.get_weights())\n",
        "\n",
        "    # @tf.function\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        self.total_steps += 1\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        q = self.online_network.predict(state)\n",
        "        return np.argmax(q, axis=1).squeeze()\n",
        "\n",
        "    # @tf.function\n",
        "    def decay_epsilon(self):\n",
        "        if self.train:\n",
        "            if self.episodes < self.epsilon_decay_steps:\n",
        "                self.epsilon -= self.epsilon_decay\n",
        "            else:\n",
        "                self.epsilon *= self.epsilon_exponential_decay\n",
        "\n",
        "    def log_progress(self):\n",
        "        self.rewards_history.append(self.episode_reward)\n",
        "        self.steps_per_episode.append(self.episode_length)\n",
        "\n",
        "        avg_steps_100 = np.mean(self.steps_per_episode[-100:])\n",
        "        avg_steps_10 = np.mean(self.steps_per_episode[-10:])\n",
        "        max_steps_10 = np.max(self.steps_per_episode[-10:])\n",
        "        avg_rewards_100 = np.mean(self.rewards_history[-100:])\n",
        "        avg_rewards_10 = np.mean(self.rewards_history[-10:])\n",
        "        max_rewards_10 = np.max(self.rewards_history[-10:])\n",
        "\n",
        "        with self.summary_writer.as_default():\n",
        "            tf.summary.scalar('Episode Reward', self.episode_reward, step=self.episodes)\n",
        "            tf.summary.scalar('Episode Rewards (MA 100)', avg_rewards_100, step=self.episodes)\n",
        "            tf.summary.scalar('Episode Steps', self.episode_length, step=self.episodes)\n",
        "            tf.summary.scalar('Epsilon', self.epsilon, step=self.episodes)\n",
        "\n",
        "        if self.episodes % self.log_every == 0:\n",
        "            template = '{:03} | {} | Rewards {:4.0f} {:4.0f} {:4.0f} | ' \\\n",
        "                       'Steps: {:4.0f} {:4.0f} {:4.0f} | Epsilon: {:.4f}'\n",
        "            print(template.format(self.episodes, format_time(time() - self.start),\n",
        "                                  avg_rewards_100, avg_rewards_10, max_rewards_10,\n",
        "                                  avg_steps_100, avg_steps_10, max_steps_10,\n",
        "                                  self.epsilon))\n",
        "\n",
        "    def memorize_transition(self, s, a, r, s_prime, done):\n",
        "        self.experience.store(s, a, r, s_prime, done)\n",
        "        self.episode_reward += r\n",
        "        self.episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            self.epsilon_history.append(self.epsilon)\n",
        "            self.decay_epsilon()\n",
        "            self.episodes += 1\n",
        "            self.log_progress()\n",
        "            self.episode_reward = 0\n",
        "            self.episode_length = 0\n",
        "\n",
        "    def experience_replay(self):\n",
        "        # not enough experience yet\n",
        "        if self.batch_size > self.experience.idx:\n",
        "            return\n",
        "\n",
        "        # sample minibatch\n",
        "        states, actions, rewards, next_states, done = self.experience.sample(self.batch_size)\n",
        "\n",
        "        # select best next action (online)\n",
        "        next_action = tf.argmax(self.online_network.predict(next_states, self.batch_size), axis=1, name='next_action')\n",
        "        # predict next q values (target)\n",
        "        next_q_values = self.target_network.predict(next_states, self.batch_size)\n",
        "        # get q values for best next action\n",
        "        target_q = (tf.math.reduce_sum(next_q_values *\n",
        "                                       tf.one_hot(next_action,\n",
        "                                                  self.num_actions),\n",
        "                                       axis=1, name='target_q'))\n",
        "        # compute td target\n",
        "        td_target = rewards + done * self.gamma * target_q\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.online_network(states)\n",
        "            q_values = tf.math.reduce_sum(q_values * tf.one_hot(actions, self.num_actions), axis=1, name='q_values')\n",
        "            loss = tf.math.reduce_mean(tf.square(td_target - q_values))\n",
        "\n",
        "        # run back propagation\n",
        "        variables = self.online_network.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        with self.summary_writer.as_default():\n",
        "            tf.summary.scalar('Loss', loss, step=self.train_steps)\n",
        "        self.train_steps += 1\n",
        "\n",
        "        if self.total_steps % self.tau == 0:\n",
        "            self.update_target()\n",
        "\n",
        "    def store_results(self):\n",
        "        result = pd.DataFrame({'Rewards': self.rewards_history,\n",
        "                               'Steps'  : self.steps_per_episode,\n",
        "                               'Epsilon': self.epsilon_history},\n",
        "                              index=list(range(1, len(self.rewards_history) + 1)))\n",
        "\n",
        "        result.to_csv(self.results_dir / 'results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 실험 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.890230Z",
          "start_time": "2021-02-25T05:54:15.881085Z"
        }
      },
      "outputs": [],
      "source": [
        "experiment = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.898305Z",
          "start_time": "2021-02-25T05:54:15.891338Z"
        }
      },
      "outputs": [],
      "source": [
        "results_dir = Path('results', 'lunar_lander', 'experiment_{}'.format(experiment))\n",
        "if not results_dir.exists():\n",
        "    results_dir.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI Gym 달 착륙선 환경 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LL 환경에서 주요 매개변수를 인스턴스화하고 추출하는 것부터 시작하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.936566Z",
          "start_time": "2021-02-25T05:54:15.899324Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "state_dim = env.observation_space.shape[0]  # number of dimensions in state\n",
        "num_actions = env.action_space.n  # number of actions\n",
        "max_episode_steps = env.spec.max_episode_steps  # max number of steps per episode\n",
        "env.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "또한 에이전트의 성과를 표시하는 비디오를 주기적으로 저장할 수 있는 내장 래퍼를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.940255Z",
          "start_time": "2021-02-25T05:54:15.937922Z"
        }
      },
      "outputs": [],
      "source": [
        "monitor_path = results_dir / 'monitor'\n",
        "video_freq = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.948582Z",
          "start_time": "2021-02-25T05:54:15.941681Z"
        }
      },
      "outputs": [],
      "source": [
        "env = wrappers.Monitor(env,\n",
        "                       directory=monitor_path.as_posix(),\n",
        "                       video_callable=lambda count: count % video_freq == 0,\n",
        "                      force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 하이퍼파라미터 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "에이전트의 성능은 여러 하이퍼파라미터에 매우 민감합니다. Q-Network, 재생 버퍼 및 ε-탐욕 정책 매개변수를 설정하기 전에 할인부터 시작하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 할인율"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.956384Z",
          "start_time": "2021-02-25T05:54:15.949684Z"
        }
      },
      "outputs": [],
      "source": [
        "gamma = .99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-네트워크 매개변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.964635Z",
          "start_time": "2021-02-25T05:54:15.957372Z"
        }
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.972118Z",
          "start_time": "2021-02-25T05:54:15.965506Z"
        }
      },
      "outputs": [],
      "source": [
        "architecture = (256, 256)  # units per layer\n",
        "l2_reg = 1e-6  # L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "100개의 시간 단계마다 대상 네트워크를 업데이트하고, 재생 메모리에 최대 1m의 과거 에피소드를 저장하고, 에이전트를 훈련하기 위해 메모리에서 1,024개의 미니 배치를 샘플링합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-12T19:00:19.808052Z",
          "start_time": "2020-04-12T19:00:19.801375Z"
        }
      },
      "source": [
        "### 재생 버퍼 매개변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.980141Z",
          "start_time": "2021-02-25T05:54:15.973195Z"
        }
      },
      "outputs": [],
      "source": [
        "tau = 100  # target network update frequency\n",
        "replay_capacity = int(1e6)\n",
        "batch_size = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ε-욕심 정책"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ε-탐욕 정책은 ε=1에서 순수 탐색으로 시작하여 100개 에피소드에 걸쳐 모든 단계에서 0.01까지 선형적으로 감소한 다음 0.99 비율로 지수적으로 감소합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:15.988320Z",
          "start_time": "2021-02-25T05:54:15.981057Z"
        }
      },
      "outputs": [],
      "source": [
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay_steps = 100\n",
        "epsilon_exponential_decay = .99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DDQN 에이전트 인스턴스화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:16.347807Z",
          "start_time": "2021-02-25T05:54:15.989364Z"
        }
      },
      "outputs": [],
      "source": [
        "agent = DDQNAgent(state_dim=state_dim,\n",
        "                  num_actions=num_actions,\n",
        "                  learning_rate=learning_rate,\n",
        "                  gamma=gamma,\n",
        "                  epsilon_start=epsilon_start,\n",
        "                  epsilon_end=epsilon_end,\n",
        "                  epsilon_decay_steps=epsilon_decay_steps,\n",
        "                  epsilon_exponential_decay=epsilon_exponential_decay,\n",
        "                  replay_capacity=replay_capacity,\n",
        "                  architecture=architecture,\n",
        "                  l2_reg=l2_reg,\n",
        "                  tau=tau,\n",
        "                  batch_size=batch_size,\n",
        "                  results_dir=results_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 훈련 및 테스트 에이전트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:16.351969Z",
          "start_time": "2021-02-25T05:54:16.348763Z"
        }
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:54:16.378089Z",
          "start_time": "2021-02-25T05:54:16.352990Z"
        }
      },
      "outputs": [],
      "source": [
        "max_episodes = 2500\n",
        "test_episodes = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "에피소드 번호, 경과 시간 외에도 지난 100회 및 최근 10회 보상 및 에피소드 길이에 대한 이동 평균과 지난 10회 반복에 대한 각각의 최대값을 기록합니다. 우리는 또한 엡실론의 붕괴를 추적합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:57:38.982373Z",
          "start_time": "2021-02-25T05:54:16.379020Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "010 | 00:00:03 | Rewards -193 -193  -70 | Steps:   93   93  143 | Epsilon: 0.9010\n",
            "020 | 00:00:29 | Rewards -157 -122  -59 | Steps:   95   97  153 | Epsilon: 0.8020\n",
            "030 | 00:00:59 | Rewards -140 -105  -43 | Steps:   95   96  129 | Epsilon: 0.7030\n",
            "040 | 00:01:37 | Rewards -129  -98  -29 | Steps:  101  116  188 | Epsilon: 0.6040\n",
            "050 | 00:02:25 | Rewards -114  -51   25 | Steps:  108  137  277 | Epsilon: 0.5050\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b27f98ec606e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemorize_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mthis_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a46dafd8269c>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'next_action'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# predict next q values (target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;31m# get q values for best next action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         target_q = (tf.math.reduce_sum(next_q_values *\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoc_controls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Runs an evaluation execution with multiple steps.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0mhas\u001b[0m \u001b[0man\u001b[0m \u001b[0munknown\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpadded_shapes\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4068\u001b[0m                     (value, output_type))\n\u001b[1;32m   4069\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4070\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_padding_values_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3221\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_NestedVariant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \"\"\"Returns a string summarizing this function's signature.\n\u001b[1;32m   2530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m       \u001b[0mdefault_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0minclude\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_to_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mkwargs_to_include\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs_to_include\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m     flatten_inputs = nest.flatten_up_to(\n\u001b[0m\u001b[1;32m   2778\u001b[0m         \u001b[0minput_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_indices_to_default_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m           ]\n\u001b[0;32m-> 2657\u001b[0;31m           raise TypeError(\"{} missing required arguments: {}\".format(\n\u001b[0m\u001b[1;32m   2658\u001b[0m               self.signature_summary(), \", \".join(missing_args)))\n\u001b[1;32m   2659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0;31m# Wrapping around a decorator allows checks like tf_inspect.getargspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         \u001b[0;31m# to be accurate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconverted_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3154\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3156\u001b[0;31m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3157\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m     self._structure = nest.map_structure(\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# dealing with the extra loop increment operation that the for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# canonicalization creates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontinue_statements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_statements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    490\u001b[0m       ' @tf.autograph.experimental.do_not_convert')\n\u001b[1;32m    491\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedLanguageElementError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcaller_fn_scope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'either caller_fn_scope or options must have a value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaller_fn_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    345\u001b[0m             indices, [num_in_full_batch], [self._partial_batch_size]))\n\u001b[1;32m    346\u001b[0m         \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_remainder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# 1024 is a magic constant that has not been properly evaluated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m   1035\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     return strided_slice(\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0mpacked_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9092\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9093\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9094\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9095\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9096\u001b[0m       return scatter_nd_non_aliasing_add_eager_fallback(\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    463\u001b[0m                               \u001b[0;34m\"earlier arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                               (prefix, dtype.name))\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m               \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that don't all match.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convert_to_tensor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m def convert_to_tensor_v2_with_dispatch(\n\u001b[1;32m   1343\u001b[0m     value, dtype=None, dtype_hint=None, name=None):\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eager_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_eager_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   raise TypeError(\"Eager execution of tf.constant with unsupported shape \"\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincorrectly\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0munsupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    430\u001b[0m           \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquint16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m       ])\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_is_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "while agent.episodes < max_episodes:\n",
        "    this_state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.memorize_transition(this_state, action, reward, next_state, done)\n",
        "        agent.experience_replay()\n",
        "        this_state = next_state\n",
        "    if np.mean(agent.rewards_history[-100:]) > 200:\n",
        "        break\n",
        "\n",
        "agent.store_results()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결과 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:57:38.984134Z",
          "start_time": "2021-02-25T05:54:14.519Z"
        }
      },
      "outputs": [],
      "source": [
        "results = pd.read_csv(results_dir / 'results.csv')\n",
        "results['MA100'] = results.rolling(window=100, min_periods=25).Rewards.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:57:38.984901Z",
          "start_time": "2021-02-25T05:54:14.522Z"
        }
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(16, 4), sharex=True)\n",
        "results[['Rewards', 'MA100']].plot(ax=axes[0])\n",
        "axes[0].set_ylabel('Rewards')\n",
        "axes[0].set_xlabel('Episodes')\n",
        "axes[0].axhline(200, c='k', ls='--', lw=1)\n",
        "results[['Steps', 'Epsilon']].plot(secondary_y='Epsilon', ax=axes[1]);\n",
        "axes[1].set_xlabel('Episodes')\n",
        "fig.suptitle('Double Deep Q-Network Agent | Lunar Lander', fontsize=16)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=.9)\n",
        "fig.savefig(results_dir / 'trading_agent_2ed', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 텐서보드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:57:38.985467Z",
          "start_time": "2021-02-25T05:54:14.525Z"
        }
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-02-25T05:57:38.986010Z",
          "start_time": "2021-02-25T05:54:14.528Z"
        }
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir results/lunar_lander/experiment_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "toc-autonumbering": true
  },
  "nbformat": 4,
  "nbformat_minor": 4
}