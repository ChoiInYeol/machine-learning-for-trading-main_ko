# 심층 강화 학습: 거래 에이전트 구축

강화 학습(RL)은 에이전트가 불완전한 정보를 가지고 있는 일반적으로 확률론적 환경과 상호 작용하는 에이전트가 수행하는 목표 지향 학습에 대한 계산적 접근 방식입니다. RL은 보상 신호에서 상태와 행동의 가치를 학습하여 에이전트가 장기적인 목표를 달성하기 위한 결정을 내리는 방법을 자동화하는 것을 목표로 합니다. 궁극적인 목표는 행동 규칙을 인코딩하고 상태를 행동에 매핑하는 정책을 도출하는 것입니다.

이 장에서는 강화학습 문제를 공식화하는 방법과 다양한 해결 방법을 적용하는 방법을 보여줍니다. 모델 기반 및 모델 없는 방법을 다루고, `step()` 환경을 소개하고, 딥 러닝과 RL을 결합하여 복잡한 환경을 탐색하는 에이전트를 교육합니다. 마지막으로 목적 함수를 최적화하려고 시도하면서 금융 시장과 상호 작용하는 에이전트를 모델링하여 알고리즘 거래에 RL을 적용하는 방법을 보여 드리겠습니다.

#### 목차

1. __자리표시자_1__
    * __자리표시자_2__
    * __자리표시자_3__
    * __자리표시자_4__
    * __자리표시자_5__
    * __자리표시자_6__
2. __자리표시자_7__
    * __자리표시자_8__
    * __자리표시자_9__
3. __자리표시자_10__
    * __자리표시자_11__
    * __자리표시자_12__
    * __자리표시자_13__
    * __자리표시자_14__
4. __자리표시자_15__
    * __자리표시자_16__
    * __자리표시자_17__
5. __자리표시자_18__
    * __자리표시자_19__
    * __자리표시자_20__

## 강화 학습 시스템의 핵심 요소

RL 문제는 지금까지 다룬 ML 설정과 차별화되는 몇 가지 요소를 갖추고 있습니다. 다음 두 섹션에서는 결정을 자동화하는 정책을 학습하여 RL 문제를 정의하고 해결하는 데 필요한 주요 기능을 간략하게 설명합니다. 
우리는 이 표기법을 사용하고 일반적으로 이 장의 범위가 허용하는 간략한 요약 이상의 추가 연구를 위해 권장되는 [강화 학습: 소개](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)(Sutton and Barto 2018) 및 David Silver의 [RL에 대한 UCL 코스](https://www.davidsilver.uk/teaching/)를 따를 것입니다.

RL 문제는 환경에 대한 객관적인 기능을 기반으로 에이전트의 결정을 최적화하는 것을 목표로 합니다.

### 상황:상태를 행동으로 변환
언제든지 정책은 에이전트의 동작을 정의합니다. 에이전트가 접할 수 있는 모든 상태를 하나 이상의 작업에 매핑합니다. 상태와 작업 수가 제한된 환경에서 정책은 훈련 중에 채워지는 간단한 조회 테이블이 될 수 있습니다.

### 보상: 행동으로부터 배우기

보상 신호는 환경이 각 시간 단계에서 에이전트에 보내는 단일 값입니다. 에이전트의 목표는 일반적으로 시간이 지남에 따라 받는 총 보상을 최대화하는 것입니다. 보상은 상태와 행동의 확률론적 기능일 수도 있습니다. 일반적으로 수렴을 촉진하고 가치의 시간적 가치 하락을 반영하기 위해 할인됩니다.
 
### 가치함수: 내부적으로 최초의 결정
보상은 행동에 대한 즉각적인 피드백을 제공합니다. 그러나 RL 문제를 해결하려면 장기적으로 가치를 창출하는 결정이 필요합니다. 이것이 바로 가치 함수가 등장하는 곳입니다. 이는 장기적인 보상 측면에서 상태 또는 특정 상태에서의 행동의 유용성을 요약합니다. 
 
### 환경
환경은 상태에 대한 정보를 에이전트에 제공하고, 행동에 대한 보상을 할당하고, 에이전트가 알 수도 있고 모를 수도 있는 확률 분포에 따라 에이전트를 새로운 상태로 전환합니다. 
이는 완전히 또는 부분적으로 관찰 가능하며 다른 에이전트를 포함할 수도 있습니다. 환경 설계에는 일반적으로 훈련 중에 에이전트의 목표 지향 학습을 촉진하기 위한 상당한 사전 설계 노력이 필요합니다.

RL 문제는 상태와 이산적이거나 연속적인 행동 공간의 복잡성에 따라 다릅니다. 후자를 사용하려면 ML이 상태, 작업 및 해당 값 간의 기능적 관계를 근사화해야 합니다. 또한 훈련 중에 에이전트가 경험하는 상태 및 작업의 하위 집합을 일반화해야 합니다.

### 대화형 RL 시스템의 구성 요소

RL 시스템의 구성 요소는 일반적으로 다음과 같습니다.

- 환경상태에 대한 대리인의 관찰
- 에이전트가 수행할 수 있는 일련의 작업
- 대리인의 결정을 지배하는 정책

또한 환경은 에이전트의 작업으로 인해 발생하는 새로운 상태를 반영하는 보상 신호를 내보냅니다. 핵심에서 에이전트는 일반적으로 행동에 대한 판단을 형성하는 가치 함수를 학습합니다. 에이전트는 보상 신호를 처리하고 가치 판단을 최적의 정책으로 변환하는 객관적인 기능을 가지고 있습니다.

## RL 문제를 해결하는 방법

RL 방법은 장기적인 목표를 달성하기 위한 조치를 취하는 방법에 대한 경험을 통해 배우는 것을 목표로 합니다. 이를 위해 에이전트와 환경은 이전 섹션에서 설명한 작업, 상태 관찰 및 보상의 인터페이스를 통해 일련의 개별 시간 단계에 걸쳐 상호 작용합니다.

에이전트의 최적 행동에 대한 규칙을 찾는 것을 의미하는 RL 문제를 해결하는 데는 여러 가지 접근 방식이 있습니다.

- **동적 프로그래밍**(DP) 방법은 환경에 대한 완전한 지식을 비현실적으로 가정하는 경우가 많지만 대부분의 다른 접근 방식의 개념적 기반입니다.
- **몬테 카를로**(MC) 방법은 전체 상태-행동-보상 시퀀스를 샘플링하여 환경과 다양한 결정의 비용 및 이점에 대해 학습합니다.
- **시간적 차이**(TD) 학습은 더 짧은 시퀀스에서 학습하여 샘플 효율성을 크게 향상시킵니다. 이를 위해 자체 사전 추정치를 기반으로 추정치를 개선하는 것으로 정의되는 부트스트래핑에 의존합니다.

연속 상태 및/또는 작업 공간에 대한 접근 방식은 ML을 활용하여 가치 또는 정책 기능을 근사화하는 경우가 많습니다. 따라서 지도 학습, 특히 지난 여러 장에서 논의한 딥 러닝 방법을 통합합니다. 그러나 이러한 방법은 RL 컨텍스트에서 뚜렷한 문제에 직면합니다.

- 보상 신호는 라벨링된 샘플과 같은 목표 개념을 직접적으로 반영하지 않습니다.
- 관찰의 분포는 에이전트의 행동과 그 자체가 학습 과정의 주제인 정책에 따라 달라집니다.

### 코드 예: 동적 프로그래밍 - 값 및 정책 반복

유한 MDP는 간단하면서도 기본적인 프레임워크입니다. 이 섹션에서는 에이전트가 최적화하려는 보상의 궤적을 소개하고, 최적화 문제를 공식화하는 데 사용되는 정책 및 가치 함수와 솔루션 방법의 기초가 되는 벨만 방정식을 정의합니다.

[그리드월드_동적_프로그래밍](01_gridworld_dynamic_programming.ipynb) 노트북은 3 x 4 그리드로 구성된 장난감 환경에 가치 및 정책 반복을 적용합니다.

### 코드 예: Q-Learning

Q-러닝은 1989년 Chris Watkins가 [박사 논문]((http://www.cs.rhul.ac.uk/~chrisw/thesis.html))를 위해 개발했을 때 초기 RL의 획기적인 발전이었습니다. 이전 섹션에서 가치 및 정책 반복에 사용한 전환 및 보상 매트릭스를 모르거나 모델링하지 않고도 MDP를 제어하기 위한 증분 동적 프로그래밍을 소개합니다. 3년 후 [왓킨스와 다얀](http://www.gatsby.ucl.ac.uk/~dayan/papers/wd92.html)에 의해 수렴 증명이 이루어졌습니다.

Q-학습은 행동-가치 함수 q를 직접 최적화하여 q*를 근사화합니다. 학습은 정책을 벗어나 진행됩니다. 즉, 알고리즘은 가치 함수만으로 암시되는 정책을 기반으로 작업을 선택할 필요가 없습니다. 그러나 수렴하려면 모든 상태-작업 쌍이 훈련 프로세스 전반에 걸쳐 계속 업데이트되어야 합니다. 이를 보장하는 간단한 방법은 ε-탐욕 정책을 사용하는 것입니다.

Q-learning 알고리즘은 주어진 에피소드 수에 대해 무작위 초기화 후 상태-동작 값 함수를 지속적으로 개선합니다. 각 시간 단계에서 ε-탐욕 정책을 기반으로 행동을 선택하고 학습률 α를 사용하여 보상과 다음 상태에 대한 가치 함수의 현재 추정치를 기반으로 가치 함수를 업데이트합니다.

[Gridworld_q_learning](02_gridworld_q_learning.ipynb) 노트북은 이전 섹션의 3 x 4 상태 그리드를 사용하여 Q-학습 에이전트를 구축하는 방법을 보여줍니다.

## 심층 강화 학습

이 섹션에서는 단순히 상태-동작 값으로 배열을 채우는 표 형식 솔루션을 사용할 수 없는 연속 상태 및 동작에 Q-Learning을 적용합니다. 대신, 수렴을 가속화하기 위해 다양한 개선을 통해 심층 Q 네트워크를 구축하기 위해 신경망을 사용하여 최적의 상태-값 함수를 근사화하는 방법을 살펴보겠습니다. 그런 다음 [오픈AI 체육관](http://gym.openai.com/docs/)을 사용하여 알고리즘을 [달착륙선](https://gym.openai.com/envs/LunarLander-v2) 환경에 적용하는 방법을 살펴보겠습니다.

### 신경망을 이용한 가치 함수 근사

다른 분야와 마찬가지로 심층 신경망은 가치 함수를 근사화하는 데 널리 사용됩니다. 그러나 ML은 (아마도 무작위로 지정된) 정책을 사용하여 모델과 환경의 상호 작용을 통해 데이터가 생성되는 RL 컨텍스트에서 뚜렷한 문제에 직면합니다.

- 연속 상태에서는 에이전트가 대부분의 상태를 방문하지 못하므로 일반화가 필요합니다.
- 지도 학습의 목표는 대표성이 있고 올바르게 라벨이 지정된 독립적이고 동일하게 분포된 샘플의 샘플을 일반화하는 것입니다. RL 컨텍스트에서는 학습이 온라인으로 이루어져야 하도록 시간 단계당 하나의 샘플만 있습니다.
- 순차적 상태가 유사하고 상태와 행동에 대한 행동 분포가 고정되어 있지 않고 에이전트 학습의 결과로 변할 때 샘플의 상관 관계가 높아질 수 있습니다.

### Deep Q-learning 알고리즘 및 확장

Deep Q 학습은 심층 신경망을 사용하여 특정 상태에 대해 사용 가능한 작업의 값을 추정합니다. 이는 RL 에이전트가 픽셀 입력만으로 게임을 플레이하는 방법을 학습한 Deep Mind의 [심층 강화 학습으로 Atari 플레이하기](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)(2013)에 의해 소개되었습니다.

심층 Q 학습 알고리즘은 상태를 행동에 매핑하는 다층 심층 Q 네트워크(DQN)의 가중치 세트 θ를 학습하여 행동-가치 함수 q를 근사화합니다.

몇 가지 혁신을 통해 심층 Q-학습의 정확성과 수렴 속도가 향상되었습니다.
- **경험 재생**은 상태, 행동, 보상 및 다음 상태 전환의 기록을 저장하고 에이전트가 ε-탐욕적인 행동을 선택하기 전에 각 시간 단계에서 네트워크 가중치를 업데이트하기 위해 이 경험에서 미니 배치를 무작위로 샘플링합니다. 이는 샘플 효율성을 높이고, 샘플의 자기 상관을 감소시키며, 로컬 최소값 또는 발산으로 이어질 수 있는 훈련 샘플을 생성하는 현재 가중치로 인해 피드백을 제한합니다.
- **천천히 변화하는 대상 네트워크**는 신경망 가중치 업데이트에 대한 현재 네트워크 매개변수의 피드백 루프를 약화시킵니다. [심층 강화 학습을 통한 인간 수준의 제어](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)(2015)에서 Deep Mind가 개발한 이 네트워크는 Q 네트워크와 동일한 아키텍처를 갖는 천천히 변화하는 대상 네트워크를 사용하지만 가중치는 주기적으로만 업데이트됩니다. 타겟 네트워크는 현재 상태 값에 대한 Q-Networks 추정을 업데이트하는 데 사용되는 다음 상태 값에 대한 예측을 생성합니다.
- **이중 딥 Q-러닝**은 의도적으로 가장 높은 액션 값을 샘플링하기 때문에 액션 값을 과대평가하는 딥 Q-러닝의 편향을 해결합니다. Hado van Hasselt가 [Double Q-learning을 이용한 심층 강화 학습](https://arxiv.org/abs/1509.06461)(2015)에서 보여준 것처럼 이러한 편견은 균일하게 적용되지 않으면 학습 과정과 결과 정책에 부정적인 영향을 미칠 수 있습니다. 작업 선택에서 작업 값 추정을 분리하기 위해 DDQN(Double Deep Q-Learning)은 한 네트워크의 가중치를 사용하여 다음 상태에서 최상의 작업을 선택하고 다른 네트워크의 가중치를 사용하여 해당 작업 값을 제공합니다. 추정.

### Open AI Gym – 달착륙선 환경

[오픈AI 체육관](https://gym.openai.com/)는 Python을 사용하여 RL 알고리즘을 테스트하고 벤치마킹하기 위한 표준화된 환경을 제공하는 RL 플랫폼입니다. 플랫폼 확장 및 맞춤형 환경 등록도 가능합니다.

[달착륙선](https://gym.openai.com/envs/LunarLander-v2) (LL) 환경에서는 에이전트가 위치, 방향 및 속도를 포함하는 이산 작업 공간과 저차원 상태 관찰을 기반으로 2차원에서 모션을 제어해야 합니다. 각 단계에서 환경은 새로운 상태에 대한 관찰과 긍정적이거나 부정적인 보상을 제공합니다. 각 에피소드는 최대 1,000개의 시간 단계로 구성됩니다.

### 코드 예: Tensorflow를 사용한 Double Deep Q-Learning

[lunar_lander_deep_q_learning](03_lunar_lander_deep_q_learning.ipynb) 노트북은 TensorFlow 및 Open AI Gym의 Lunar Lander 환경을 사용하는 DDQN 에이전트를 구현합니다.

## 코드 예: TensorFlow 2 및 OpenAI Gym과의 거래를 위한 심층 RL

트레이딩 에이전트를 교육하려면 가격 및 기타 정보를 제공하고 트레이딩 관련 액션을 제공하며 그에 따라 에이전트에게 보상할 수 있도록 포트폴리오를 추적하는 시장 환경을 조성해야 합니다.

### OpenAI 거래 환경을 설계하는 방법

OpenAI Gym은 [선적 서류 비치](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym)에 설명된 대로 아키텍처를 준수하는 환경의 설계, 등록 및 활용을 허용합니다. 
- [trading_env.py](trading_env.py) 파일은 필수 [오픈AI 체육관](https://gym.openai.com/) 및 `reset()` 메서드를 구현하는 클래스를 생성하는 방법을 보여주는 예제를 구현합니다.

거래 환경은 에이전트의 활동을 촉진하기 위해 상호 작용하는 세 가지 클래스로 구성됩니다.
 1. `DataSource` 클래스는 시계열을 로드하고 몇 가지 기능을 생성하며 각 시간 단계에서 에이전트에 최신 관찰을 제공합니다. 
 2. `TradingSimulator`은 포지션, 거래, 비용, 성과를 추적합니다. 또한 매수 후 보유 벤치마크 전략의 결과를 구현하고 기록합니다. 
 3. `TradingEnvironment` 자체가 프로세스를 조정합니다. 
 
### 주식 시장을 묘사하는 Deep Q-learning 에이전트를 구축하는 방법
 
[q_learning_for_trading](04_q_learning_for_trading.ipynb) 노트북은 제한된 옵션 세트, 상대적으로 낮은 차원 상태 및 [lunar_lander_deep_q_learning](03_lunar_lander_deep_q_learning.ipynb)에서 사용되는 Deep Q-Learning 에이전트를 교육하기 위해 쉽게 수정 및 확장할 수 있는 기타 매개변수를 사용하여 간단한 게임을 설정하는 방법을 보여줍니다.
 
<p 정렬="중앙">
<img src="https://i.imgur.com/lg0ofbZ.png" width="60%">
</p>

## 자원

- [강화 학습: 소개, 2판](http://incompleteideas.net/book/RLbook2018.pdf), Richard S. Sutton 및 Andrew G. Barto, 2018
- [강화 학습에 관한 University College of London 과정](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), 데이비드 실버, 2015
- [강화 학습 알고리즘 구현](https://github.com/dennybritz/reinforcement-learning), 데니 브리츠
    - 이 저장소는 널리 사용되는 강화 학습 알고리즘에 대한 코드, 연습 및 솔루션을 제공합니다. 이는 Sutton/Baron 및 Silver(위 참조)의 이론적 자료를 보완하는 학습 도구 역할을 하기 위한 것입니다.

### RL 알고리즘

- Q러닝
    - [지연된 보상으로부터 배우기](http://www.cs.rhul.ac.uk/~chrisw/thesis.html), 박사학위 논문, Chris Watkins, 1989
    - [Q-러닝](http://www.gatsby.ucl.ac.uk/~dayan/papers/wd92.html), 기계 학습, 1992
- 딥큐네트웍스
    - [심층 강화 학습으로 Atari 플레이하기](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih 외, 2013
    - 강화학습을 활용하여 고차원 감각 입력으로부터 직접 제어 정책을 성공적으로 학습하는 최초의 딥러닝 모델을 제시합니다. 이 모델은 입력이 원시 픽셀이고 출력이 미래 보상을 추정하는 가치 함수인 Q-learning의 변형으로 훈련된 컨볼루셔널 신경망입니다. 우리는 아키텍처나 학습 알고리즘을 조정하지 않고 아케이드 학습 환경의 Atari 2600 게임 7개에 우리 방법을 적용했습니다. 우리는 이 게임이 6개 게임에서 이전의 모든 접근 방식보다 성능이 뛰어나고 그 중 3개 게임에서는 인간 전문가를 능가한다는 사실을 발견했습니다.
- 비동기적 장점 배우-비평가(A2C/A3C)
    - [심층 강화 학습을 위한 비동기식 방법](https://arxiv.org/abs/1602.01783), Mnih, V. 외. 2016년
    - 심층 신경망 컨트롤러의 최적화를 위해 비동기 경사하강법을 사용하는 심층 강화 학습을 위한 개념적으로 간단하고 가벼운 프레임워크를 제안합니다. 우리는 네 가지 표준 강화 학습 알고리즘의 비동기 변형을 제시하고 병렬 행위자-학습자가 네 가지 방법 모두 신경망 컨트롤러를 성공적으로 훈련할 수 있도록 훈련에 안정화 효과가 있음을 보여줍니다. 배우 평론가의 비동기 변형인 최고 성능 방법은 GPU 대신 단일 멀티 코어 CPU에서 절반의 시간 동안 훈련하는 동시에 Atari 도메인의 현재 최첨단 기술을 능가합니다. 또한, 우리는 비동기 배우 평론가가 시각적 입력을 사용하여 무작위 3D 미로를 탐색하는 새로운 작업뿐만 아니라 다양한 연속 모터 제어 문제에서 성공한다는 것을 보여줍니다.
- 근접 정책 최적화(PPO)
    - [근위 정책 최적화 알고리즘](https://arxiv.org/abs/1707.06347), Schulman 외, 2017
    - 환경과의 상호 작용을 통해 데이터 샘플링을 번갈아 수행하고 확률적 경사 상승을 사용하여 "대리" 목적 함수를 최적화하는 강화 학습을 위한 새로운 정책 경사 방법 제품군을 제안합니다. 표준 정책 경사 방법은 데이터 샘플당 하나의 경사 업데이트를 수행하는 반면, 우리는 미니배치 업데이트의 여러 시대를 가능하게 하는 새로운 목적 함수를 제안합니다. PPO(근위 정책 최적화)라고 하는 새로운 방법은 TRPO(신뢰 영역 정책 최적화)의 일부 이점을 가지고 있지만 구현이 훨씬 간단하고 더 일반적이며 경험적으로 더 나은 샘플 복잡성을 갖습니다. 우리의 실험은 시뮬레이션된 로봇 운동 및 Atari 게임 플레이를 포함한 일련의 벤치마크 작업에 대해 PPO를 테스트하고 PPO가 다른 온라인 정책 그라데이션 방법보다 성능이 뛰어나며 전반적으로 샘플 복잡성, 단순성 및 벽 시간 간의 유리한 균형을 유지한다는 것을 보여줍니다.

- 신뢰 영역 정책 최적화(TRPO)
    - [신뢰 영역 정책 최적화](https://arxiv.org/abs/1502.05477), Schulman 외, 2015
    - 단조로운 개선이 보장된 정책 최적화를 위한 반복적 절차를 설명합니다. 이론적으로 정당화된 절차에 대한 몇 가지 근사치를 만들어 TRPO(Trust Region Policy Optimization)라는 실용적인 알고리즘을 개발합니다. 이 알고리즘은 자연적인 정책 기울기 방법과 유사하며 신경망과 같은 대규모 비선형 정책을 최적화하는 데 효과적입니다. 우리의 실험은 시뮬레이션된 로봇 수영, 호핑 및 걷기 보행 학습과 같은 다양한 작업에서 강력한 성능을 보여줍니다. 화면 이미지를 입력으로 사용하여 Atari 게임을 플레이합니다. 이론에서 벗어난 근사치에도 불구하고 TRPO는 하이퍼파라미터를 거의 조정하지 않고도 단조로운 개선을 제공하는 경향이 있습니다.
    
- 심층 결정론적 정책 변화도(DDPG)
    - [심층 강화 학습을 통한 지속적인 제어](https://arxiv.org/abs/1509.02971), Lillicrap 외, 2015
    - Deep Q-Learning의 성공 기반이 되는 아이디어를 지속적인 행동 영역에 적용합니다. 우리는 연속적인 행동 공간에서 작동할 수 있는 결정론적 정책 그라데이션을 기반으로 하는 배우 평론가의 모델 없는 알고리즘을 제시합니다. 동일한 학습 알고리즘, 네트워크 아키텍처 및 하이퍼 매개변수를 사용하여 당사의 알고리즘은 카트폴 스윙업, 능숙한 조작, 다리 이동 및 자동차 운전과 같은 고전적인 문제를 포함하여 20개 이상의 시뮬레이션된 물리 작업을 강력하게 해결합니다. 우리의 알고리즘은 도메인 및 파생 상품의 역학에 대한 전체 액세스 권한을 가진 계획 알고리즘에서 찾은 정책과 성능이 경쟁력 있는 정책을 찾을 수 있습니다. 우리는 또한 많은 작업에 대해 알고리즘이 원시 픽셀 입력에서 직접 정책을 엔드투엔드로 학습할 수 있음을 보여줍니다.
- 트윈 지연 DDPG(TD3)
    - [배우 평론가 방법의 함수 근사 오류 해결](https://arxiv.org/abs/1802.09477), Fujimoto 외, 2018
    - Deep Q-learning과 같은 가치 기반 강화 학습 방법에서는 함수 근사 오류로 인해 가치 추정이 과대평가되고 차선책이 되는 것으로 알려져 있습니다. 우리는 이 문제가 배우 평론가 환경에서 지속된다는 것을 보여주고 배우와 평론가 모두에게 미치는 영향을 최소화하기 위한 새로운 메커니즘을 제안합니다. 우리의 알고리즘은 과대평가를 제한하기 위해 한 쌍의 비평가 사이의 최소값을 취함으로써 Double Q-learning을 기반으로 합니다. 우리는 대상 네트워크와 과대평가 편향 사이의 연관성을 도출하고, 업데이트별 오류를 줄이고 성능을 더욱 향상시키기 위해 정책 업데이트를 지연할 것을 제안합니다. 우리는 테스트된 모든 환경에서 최첨단 기술을 능가하는 OpenAI 체육관 작업 제품군에 대한 방법을 평가합니다.
- 소프트 액터 비평가(SAC)
    - [소프트 액터 비평가: 확률적 액터를 사용한 비정책 최대 엔트로피 심층 강화 학습](https://arxiv.org/abs/1801.01290), Haarnoja 외, 2018
    - 모델이 필요 없는 심층 강화 학습(RL) 알고리즘은 다양한 까다로운 의사 결정 및 제어 작업에서 입증되었습니다. 그러나 이러한 방법은 일반적으로 매우 높은 샘플 복잡성과 취약한 수렴 특성이라는 두 가지 주요 과제로 인해 세심한 하이퍼파라미터 조정이 필요합니다. 이러한 두 가지 과제 모두 이러한 방법을 복잡한 실제 영역에 적용하는 것을 심각하게 제한합니다. 본 논문에서는 최대 엔트로피 강화학습 프레임워크를 기반으로 하는 정책 외 행위자-비평가 심층 RL 알고리즘인 소프트 행위자-비평가를 제안합니다. 이 프레임워크에서 행위자는 엔트로피를 최대화하는 동시에 예상 보상을 최대화하는 것을 목표로 합니다. 즉, 가능한 한 무작위로 행동하면서 작업을 성공시키는 것입니다. 이 프레임워크를 기반으로 한 이전의 Deep RL 방법은 Q-learning 방법으로 공식화되었습니다. 정책 외 업데이트와 안정적인 확률론적 행위자 비평 공식을 결합함으로써 우리의 방법은 다양한 연속 제어 벤치마크 작업에서 최첨단 성능을 달성하고 이전 정책 및 정책 외 방법보다 뛰어난 성능을 발휘합니다. 또한, 우리는 다른 정책 외 알고리즘과 달리 우리의 접근 방식이 매우 안정적이어서 다양한 무작위 시드에서 매우 유사한 성능을 달성한다는 것을 보여줍니다.
- 범주형 51-Atom DQN(C51)
    - [강화 학습에 대한 분포 관점](https://arxiv.org/abs/1707.06887), Bellemare 외 2017
    - 본 논문에서는 가치 분포, 즉 강화 학습 에이전트가 받은 무작위 반환의 분포의 근본적인 중요성에 대해 논의합니다. 이는 이러한 수익 또는 가치에 대한 기대를 모델링하는 강화 학습에 대한 일반적인 접근 방식과 대조됩니다. 가치 분포를 연구하는 확립된 문헌이 있지만 지금까지는 위험 인식 행동 구현과 같은 특정 목적을 위해 항상 사용되어 왔습니다. 우리는 정책 평가와 통제 설정 모두에서 이론적 결과로 시작하여 후자의 심각한 분배 불안정성을 드러냅니다. 그런 다음 분포 관점을 사용하여 대략적인 값 분포 학습에 Bellman 방정식을 적용하는 새로운 알고리즘을 설계합니다. 우리는 Arcade Learning Environment의 게임 제품군을 사용하여 알고리즘을 평가합니다. 우리는 근사 강화 학습에서 가치 분포의 중요성을 보여주는 최첨단 결과와 일화적인 증거를 모두 얻습니다. 마지막으로, 우리는 이론적 및 경험적 증거를 결합하여 가치 분포가 대략적인 환경에서 학습에 영향을 미치는 방식을 강조합니다.
    
### 투자 신청
- [금융 포트폴리오 관리 문제에 대한 심층 강화 학습 프레임워크](https://arxiv.org/abs/1706.10059), Zhengyao Jiang, Dixing Xu, Jinjun Liang 2017
    - 금융 포트폴리오 관리는 자금을 다양한 금융 상품으로 지속적으로 재분배하는 프로세스입니다. 본 논문에서는 포트폴리오 관리 문제에 대한 심층적인 머신러닝 솔루션을 제공하기 위해 재무 모델이 필요 없는 강화 학습 프레임워크를 제시합니다. 프레임워크는 EIIE(Ensemble of Identical Independent Evaluators) 토폴로지, PVM(Portfolio-Vector Memory), OSBL(Online Stochastic Batch Learning) 체계 및 완전히 활용되고 명시적인 보상 기능으로 구성됩니다. 이 프레임워크는 CNN(Convolutional Neural Network), 기본 RNN(Recurrent Neural Network) 및 LSTM(Long Short-Term Memory)을 사용하여 이 작업에서 세 순간에 구현됩니다. 이는 최근 검토되거나 발표된 다수의 포트폴리오 선택 전략과 함께 암호화폐 시장에서 30분의 거래 기간을 대상으로 한 세 가지 백테스트 실험을 통해 조사되었습니다. 암호화폐는 정부가 발행한 화폐에 대한 전자적이고 분산화된 대안이며, 비트코인은 암호화폐의 가장 잘 알려진 예입니다. 프레임워크의 세 가지 인스턴스 모두 모든 실험에서 상위 3개 위치를 독점하여 다른 비교 거래 알고리즘을 능가합니다. 백테스트에서 0.25%의 높은 수수료율을 보였지만 프레임워크는 50일 안에 최소 4배의 수익을 달성할 수 있습니다.
    - __자리표시자_53__; 해당 GitHub 저장소
- [게임으로서의 금융 거래: 심층 강화 학습 접근 방식](https://arxiv.org/pdf/1807.02787.pdf), 황첸이, 2018
- __자리표시자_55__
    - CTC-Executioner는 강화 학습 기술을 사용하여 암호화폐 시장에서 지정가 주문에 대한 주문형 실행/배치 전략을 제공하는 도구입니다. 기본 프레임워크는 주문장 데이터를 분석하고 해당 기능을 파생할 수 있는 기능을 제공합니다. 그런 다음 이러한 결과를 사용하여 실행 전략의 의사 결정 프로세스를 동적으로 업데이트할 수 있습니다.
    - 사용되는 방법은 현재 TU Delft에서 진행 중인 연구 프로젝트(석사 논문)를 기반으로 합니다.
    
- __자리표시자_56__
    - (단기) 주식거래에 Q-learning을 적용한 구현입니다. 이 모델은 종가의 n일 창을 사용하여 주어진 시간에 취할 수 있는 최선의 조치가 매수, 매도 또는 매수인지 여부를 결정합니다. 단기 상태 표현의 결과로 모델은 장기 추세에 대한 결정을 내리는 데는 그다지 좋지 않지만 최고점과 최저점을 예측하는 데는 상당히 좋습니다.