# 비지도 학습: 데이터 기반 위험 요소에서 계층적 위험 동등성까지

비지도 학습은 데이터 세트에 기능만 포함되어 있고 결과 측정이 없는 경우 또는 결과와 독립적인 정보를 추출하려는 경우에 유용합니다. 미래 결과를 예측하는 대신, 목표는 데이터 세트 탐색을 포함하여 다른 작업을 해결하는 데 유용한 데이터의 유익한 표현을 학습하는 것입니다. 예를 들어 문서를 요약하기 위한 주제 식별([제14장](../14_topic_modeling) 참조), 지도 학습에 대한 과적합 및 계산 비용 위험을 낮추기 위한 기능 수 감소 또는 자산 할당을 위한 클러스터링 사용으로 설명된 유사한 관찰을 그룹화하는 것 등이 있습니다. 이 장의 끝.

차원 축소 및 클러스터링은 비지도 학습의 주요 작업입니다. 
- 차원 축소는 정보 손실을 최소화하면서 기존 기능을 새롭고 작은 세트로 변환합니다. 정보 손실을 측정하는 방법, 선형 또는 비선형 변환을 적용하는지 여부, 새로운 기능 세트에 부과하는 제약 조건에 따라 다양한 알고리즘이 존재합니다. 
- 클러스터링 알고리즘은 새로운 특징을 식별하는 대신 유사한 관찰 또는 특징을 식별하고 그룹화합니다. 알고리즘은 관찰의 유사성을 정의하는 방법과 결과 그룹에 대한 가정이 다릅니다.

보다 구체적으로 이 장에서는 다음을 다룹니다.
- 주성분 분석(PCA 및 ICA)이 선형 차원 축소를 수행하는 방법
- PCA를 활용하여 자산수익률로부터 데이터 기반의 위험요인 및 고유포트폴리오를 식별
- 매니폴드러닝을 활용하여 비선형, 고차원 데이터를 효과적으로 시각화
- T-SNE, UMAP을 활용하여 고차원 영상 데이터 탐색
- k-평균, 계층적, 밀도 기반 클러스터링 알고리즘의 작동 방식
- 집적 클러스터링을 사용하여 계층적 위험 패리티를 갖춘 강력한 포트폴리오 구축

## 콘텐츠

1. __자리표시자_1__
2. __자리표시자_2__
    * __자리표시자_3__
        - __자리표시자_4__
        - __자리 표시자_5__
    * __자리표시자_6__
3. __자리표시자_7__
    * __자리표시자_8__
    * __자리표시자_9__
    * __자리표시자_10__
4. __자리표시자_11__
5. __자리표시자_12__
    * __자리표시자_13__
    * __자리표시자_14__
    * __자리표시자_15__
6. __자리표시자_16__
    * __자리표시자_17__
    * __자리표시자_18__
7. __자리표시자_19__
    * __자리표시자_20__
    * __자리표시자_21__
        - __자리표시자_22__
        - __자리표시자_23__
    * __자리표시자_24__
    * __자리표시자_25__
    * __자리표시자_26__
    * __자리표시자_27__
        - __자리표시자_28__
        - __자리표시자_29__
    * __자리표시자_30__

## 코드 예: 차원의 저주

각각의 새로운 차원이 결과에 관한 신호를 추가할 수 있기 때문에 데이터세트의 차원 수가 중요합니다. 그러나 차원성의 저주라고 알려진 단점도 있습니다. 관측값 수가 일정하게 유지되는 동안 독립적인 특성의 수가 늘어나면 데이터 포인트 사이의 평균 거리도 늘어나고 특성 공간의 밀도가 기하급수적으로 떨어집니다. 관찰이 더 멀리 있을 때, 즉 서로 다를 때 예측이 훨씬 더 어려워지기 때문에 기계 학습에 대한 의미는 극적입니다.

노트북 [차원의 저주](01_linear_dimensionality_reduction/00_curse_of_dimensionality.ipynb)은 차원 수가 증가함에 따라 데이터 포인트 간의 평균 및 최소 거리가 어떻게 증가하는지 시뮬레이션합니다.

## 선형 차원 감소

선형 차원 축소 알고리즘은 새 기능의 특성에 대한 제약 조건에 따라 데이터의 상당한 변화를 포착하기 위해 원래 기능을 변환, 회전 및 크기 조정하는 선형 조합을 계산합니다.

이 섹션에서는 이 두 가지 알고리즘을 소개한 다음 자산 수익률에 PCA를 적용하여 데이터에서 위험 요소를 학습하고 체계적인 거래 전략을 위해 소위 고유 포트폴리오를 구축하는 방법을 설명합니다.

- [차원 축소: 가이드 투어](https://www.microsoft.com/en-us/research/publication/dimension-reduction-a-guided-tour-2/), Chris J.C. Burges, 기계 학습의 기초 및 추세, 2010년 1월

### 코드 예: 주성분 분석

PCA는 기존 기능의 선형 조합으로 주성분을 찾고 이러한 구성 요소를 사용하여 원본 데이터를 나타냅니다. 구성 요소 수는 대상 차원을 결정하는 초매개변수이며 관측값 또는 열 수 중 더 작은 수와 같거나 작아야 합니다.

#### PCA의 핵심 아이디어 시각화

노트북 [pca_key_ideas](01_linear_dimensionality_reduction/01_pca_key_ideas.ipynb)은 주요 구성 요소를 2D 및 3D로 시각화합니다.

PCA는 데이터의 분산을 대부분 캡처하여 원래 기능을 쉽게 복구하고 각 구성 요소에 정보를 추가하는 것을 목표로 합니다. 원본 데이터를 주성분 공간에 투영하여 차원성을 줄입니다. PCA는 명심해야 할 몇 가지 중요한 가정을 합니다. 여기에는 다음이 포함됩니다.
- 높은 분산은 높은 신호 대 잡음비를 의미합니다.
- 데이터는 표준화되어 특성 전반에 걸쳐 분산을 비교할 수 있습니다.
- 선형 변환은 데이터의 관련 측면을 포착합니다.
- 첫 번째와 두 번째 순간 이후의 고차 통계는 중요하지 않습니다. 이는 데이터가 정규 분포를 가짐을 의미합니다.

첫 번째와 두 번째 순간에 대한 강조는 표준 위험/수익 지표와 일치하지만 정규성 가정은 시장 데이터의 특성과 충돌할 수 있습니다.

#### PCA 알고리즘의 작동 방식

[the_math_behind_pca](01_linear_dimensionality_reduction/02_the_math_behind_pca.ipynb) 노트북은 주성분 계산을 보여줍니다.

### 참고자료

- [확률적 주성분 분석기의 혼합물](http://www.miketipping.com/papers/met-mppca.pdf), Michael E. Tipping 및 Christopher M. Bishop, 신경 계산 11(2), 443-482페이지. MIT 언론
- [무작위성을 사용하여 구조 찾기: 대략적인 행렬 분해를 구성하기 위한 확률적 알고리즘](http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf), N. Halko†, P. G. Martinsson, J. A. Tropp, SIAM REVIEW, Vol. 53, No. 2, pp. 217–288
- [SVD와 PCA의 관계. SVD를 사용하여 PCA를 수행하는 방법은 무엇입니까?](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca), 시각화를 통한 우수한 기술 CrossValidated StackExchange 답변

## 코드 예: 거래용 PCA

PCA는 자산 수익률에 PCA를 적용하여 데이터 기반 위험 요소 도출, 자산 수익률 상관 행렬의 주요 구성 요소를 기반으로 상관 관계 없는 포트폴리오 구성 등 여러 측면에서 알고리즘 거래에 유용합니다.
 
### 데이터에 의한 위험 요인

[07장 - 선형 모델](../07_linear_models/02_fama_macbeth.ipynb)에서는 수익의 주요 동인을 포착하기 위해 양적 금융에 사용되는 위험 요소 모델을 탐색했습니다. 이 모델은 체계적인 위험 요인에 대한 노출과 이러한 요인과 관련된 보상에 따라 자산 수익률의 차이를 설명합니다.
 
특히, 우리는 평균 수익률의 실증적 행동에 대한 사전 지식을 기반으로 요인을 지정하고 이러한 요인을 관찰 가능한 것으로 처리한 다음 선형 회귀를 사용하여 위험 모델 계수를 추정하는 Fama-French 접근 방식을 탐색했습니다. 대체 접근 방식은 위험 요인을 잠재 변수로 취급하고 PCA와 같은 요인 분석 기술을 사용하여 요인과 드라이브가 과거 수익에서 어떻게 돌아오는지 동시에 추정합니다.

- 노트북 [pca_and_risk_factor_models](01_linear_dimensionality_reduction/03_pca_and_risk_factor_models.ipynb)은 이 방법이 자산 수익 동작에 대한 사전 지식이 필요하지 않다는 장점과 함께 순전히 통계적 또는 데이터 기반 방식으로 요소를 도출하는 방법을 보여줍니다.
 
### 자신의 포트

PCA의 또 다른 적용에는 정규화된 수익의 공분산 행렬이 포함됩니다. 상관 행렬의 주요 구성 요소는 내림차순으로 자산 간 공변량의 대부분을 포착하며 상호 상관 관계가 없습니다. 또한 표준화된 주성분을 포트폴리오 가중치로 사용할 수 있습니다.

[pca_and_eigen_portfolios](01_linear_dimensionality_reduction/04_pca_and_eigen_portfolios.ipynb) 노트북은 고유 포트폴리오를 만드는 방법을 보여줍니다.

### 참고자료

- [특징은 공분산: 위험과 수익의 통합 모델](http://www.nber.org/2018LTAM/kelly.pdf), Kelly, Pruitt 및 Su, NBER, 2018
- [미국 주식 시장의 통계적 차익거래](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf), 마르코 아베야네다, 이정현, 2008

## 독립 성분 분석

ICA(Independent Component Analysis)는 원본 데이터를 표현하기 위해 새로운 기반을 식별하지만 PCA와는 다른 목표를 추구하는 또 다른 선형 알고리즘입니다. 자세한 소개는 [히베리넨과 오자](https://www.sciencedirect.com/science/article/pii/S0893608000000265)(2000)을 참조하세요.
 
ICA는 신호처리 분야에서 등장했으며, ICA가 해결하려는 문제를 블라인드 소스 분리(Blind Source Separation)라고 합니다. 이는 일반적으로 특정 수의 손님이 동시에 발언하여 단일 마이크가 중복되는 신호를 녹음하는 칵테일 파티 문제로 구성됩니다. ICA는 스피커 수만큼 다양한 마이크가 있다고 가정하며, 각 마이크는 서로 다른 위치에 배치되어 서로 다른 신호 혼합을 녹음합니다. 그런 다음 ICA는 다양한 녹음에서 개별 신호를 복구하는 것을 목표로 합니다.

- [독립 성분 분석: 알고리즘 및 응용](https://www.sciencedirect.com/science/article/pii/S0893608000000265), Aapo Hyvärinen 및 Erkki Oja, 신경망, 2000
- [독립 구성 요소 분석](http://cs229.stanford.edu/notes/cs229-notes11.pdf), CS229 강의 노트, Andrew Ng
- [가격, 주문 흐름, 유동성의 공통 요소](https://www.sciencedirect.com/science/article/pii/S0304405X0000091X), Hasbrouck 및 Seppi, 금융경제학 저널, 2001
- [ICA-GARCH 모델을 이용한 다변량 금융시계열의 변동성 모델링](https://link.springer.com/chapter/10.1007/11508069_74), Edmond H. C. Wu, Philip L. H. Yu, in: Gallagher M., Hogan J.P., Maire F. (eds) 지능형 데이터 엔지니어링 및 자동화 학습 - IDEAL 2005
- [독립 요인 모델의 예측 성능](http://www.cs.cuhk.hk/~lwchan/papers/icapred.pdf), Chan, In: 신경망에 관한 2002 IEEE 국제 합동 컨퍼런스 회의록
- [독립 성분 분석 및 그 응용 개요](http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/334/333), Ganesh R. Naik, Dinesh K Kumar, Informatica 2011

## 다양한 학습

다양체 가설은 고차원 데이터가 고차원 공간에 내장된 저차원 비선형 다양체 위나 근처에 있는 경우가 많다는 점을 강조합니다.

[다양한 학습](https://scikit-learn.org/stable/modules/manifold.html)은 고유한 차원의 다양성을 찾은 다음 이 하위 공간에 데이터를 표현하는 것을 목표로 합니다. 단순화된 예에서는 도로를 3차원 공간의 1차원 다양체로 사용하고 집 번호를 로컬 좌표로 사용하여 데이터 포인트를 식별합니다.

### 코드 예: 다양체의 모습

노트북 [다양한_학습_소개](02_manifold_learning/01_manifold_learning_intro.ipynb)에는 다양체의 토폴로지 구조를 보여주는 2차원 스위스 롤을 포함하여 여러 가지 예가 포함되어 있습니다.

### 코드 예: 로컬 선형 임베딩

여러 기술이 저차원 다양체에 근접합니다. 한 가지 예는 Sam Roweis와 Lawrence Saul이 2000년에 개발한 [국소 선형 임베딩](https://cs.nyu.edu/~roweis/lle/)(LLE)입니다.
 
- 노트북 [매니폴드_학습_lle](02_manifold_learning/02_manifold_learning_lle.ipynb)은 스위스 롤을 '펼치는' 방법을 보여줍니다. 각 데이터 포인트에 대해 LLE는 주어진 수의 최근접 이웃을 식별하고 각 포인트를 이웃의 선형 조합으로 나타내는 가중치를 계산합니다. 이는 저차원 다양체의 전역 내부 좌표에 각 이웃을 선형으로 투영하여 저차원 임베딩을 찾고 일련의 PCA 애플리케이션으로 생각할 수 있습니다.

일반 예시에서는 다음 데이터 세트를 사용합니다.

- __자리표시자_54__
- __자리표시자_55__

### 참고자료

- [로컬 선형 임베딩](https://cs.nyu.edu/~roweis/lle/), Sam T. Roweis 및 Lawrence K. Saul(LLE 작성자 웹사이트)

## 코드 예시: 매니폴드 학습을 통해 고차원 이미지 및 자산 가격 데이터 시각화

### t-분산 확률적 이웃 임베딩(t-SNE)

[t-SNE](https://lvdmaaten.github.io/tsne/)은 고차원 데이터의 패턴을 감지하기 위해 Laurens van der Maaten과 Geoff Hinton이 2010년에 개발한 수상 경력이 있는 알고리즘입니다. 여러 가지 다르지만 관련된 저차원 다양체에서 데이터를 찾는 데는 확률론적, 비선형 접근 방식이 필요합니다. 이 알고리즘은 높은 차원에서 떨어져 있는 점 사이의 거리를 유지하는 것과는 반대로 낮은 차원에서 유사한 점을 함께 유지하는 것을 강조합니다. 이는 제곱 거리를 최소화하는 PCA와 같은 알고리즘의 결과입니다.

- [t-SNE를 사용하여 데이터 시각화](http://www.cs.toronto.edu/~hinton/absps/tsne.pdf), van der Maaten, Hinton, 기계 학습 연구 저널, 2008
- [동적 t-SNE를 사용하여 시간 종속 데이터 시각화](http://www.cs.rug.nl/~alext/PAPERS/EuroVis16/paper.pdf), Rauber, Falcão, Telea, Eurographics 시각화 컨퍼런스(EuroVis) 2016
- [t-분산 확률적 이웃 임베딩으로 Merck Viz 챌린지에서 승리](http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/), Kaggle 블로그 2016
- [t-SNE: Google 기술 토크](https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw), 반 데르 마텐, 2013
- [파라메트릭 t-SNE](https://github.com/kylemcdonald/Parametric-t-SNE), Kyle McDonald의 Keras를 사용한 빠른 t-SNE 구현

### UMAP

[UMAP](https://github.com/lmcinnes/umap))은 시각화 및 일반 차원 축소를 위한 최신 알고리즘입니다. 데이터가 로컬로 연결된 다양체에 균일하게 분포되어 있다고 가정하고 퍼지 토폴로지를 사용하여 가장 가까운 저차원 등가물을 찾습니다. 위의 Perplexity와 유사하게 결과에 영향을 미치는 이웃 매개변수를 사용합니다.

t-SNE보다 더 빠르므로 대규모 데이터 세트에 더 잘 확장되며 때로는 t-SNE보다 전역 구조를 더 잘 보존합니다. 또한 단어 수 벡터 간의 거리를 측정하는 데 사용되는 코사인 유사성을 비롯한 다양한 거리 함수와 함께 작동할 수도 있습니다.

- [UMAP: 차원 축소를 위한 균일 다양체 근사화 및 투영](https://arxiv.org/abs/1802.03426), Leland McInnes, 존 힐리, 2018

- [매니폴드_학습_tsne_umap](02_manifold_learning/03_manifold_learning_tsne_umap.ipynb) 및 [매니폴드_학습_자산_가격](02_manifold_learning/04_manifold_learning_asset_prices.ipynb) 노트북은 주식 수익을 포함한 다양한 데이터 세트와 함께 t-SNE 및 UMAP의 사용법을 보여줍니다.

## 클러스터 알고리즘

클러스터링과 차원 축소 모두 데이터를 요약합니다. 차원 축소는 가장 관련성이 높은 정보를 캡처하는 더 적은 수의 새로운 기능을 사용하여 데이터를 표현함으로써 데이터를 압축합니다. 이와 대조적으로 클러스터링 알고리즘은 유사한 데이터 포인트로 구성된 하위 그룹에 기존 관측치를 할당합니다.

클러스터링은 연속 변수에서 학습된 범주의 렌즈를 통해 데이터를 더 잘 이해하는 데 도움이 될 수 있습니다. 또한 학습된 기준에 따라 새 개체를 자동으로 분류할 수 있습니다. 관련 애플리케이션의 예로는 계층적 분류, 의료 진단 또는 고객 세분화가 있습니다. 또는 클러스터를 사용하여 그룹을 프로토타입으로 나타낼 수 있습니다. 학습된 그룹화를 가장 잘 대표하는 클러스터의 중간 지점입니다. 예제 애플리케이션에는 이미지 압축이 포함되어 있습니다.

클러스터링 알고리즘은 그룹화 식별 전략과 관련하여 다릅니다.
- 조합 알고리즘은 서로 다른 관찰 그룹 중 가장 일관성 있는 그룹을 선택합니다.
- 확률론적 모델링은 클러스터를 생성할 가능성이 가장 높은 분포를 추정합니다.
- 계층적 클러스터링은 특정 단계에서 일관성을 최적화하는 일련의 중첩 클러스터를 찾습니다.

알고리즘은 또한 데이터 특성, 도메인 및 애플리케이션 목표와 일치해야 하는 유용한 객체 컬렉션을 구성하는 개념에 따라 다릅니다. 그룹화 유형은 다음과 같습니다.
- 다양한 모양의 그룹이 명확하게 구분되어 있음
- 프로토타입 또는 중앙 기반의 컴팩트 클러스터
- 임의 모양의 밀도 기반 클러스터
- 연결성 또는 그래프 기반 클러스터

클러스터링 알고리즘의 중요한 추가 측면에는 다음이 포함됩니다. 
- 독점적인 클러스터 멤버십이 필요합니다. 
- 하드(즉, 이진 또는 소프트, 확률적 할당)를 만듭니다. 
- 완료되었으며 모든 데이터 포인트를 클러스터에 할당합니다.

### 코드 예: 클러스터 알고리즘 비교

노트북 [클러스터링_급여](03_clustering_algorithms/01_clustering_algos.ipynb)은 클러스터링 알고리즘을 테스트하기 위해 설계된 장난감 데이터 세트를 사용하여 여러 알고리즘에 대한 클러스터링 결과를 비교합니다.

### 코드 예: k-평균

k-Means는 가장 잘 알려진 클러스터링 알고리즘으로 1957년 Bell Labs의 Stuart Lloyd가 처음 제안했습니다.

#### 알고리즘

알고리즘은 클러스터 내 분산(관성이라고 함)을 최소화하는 것을 목표로 K개의 중심을 찾고 각 데이터 포인트를 정확히 하나의 클러스터에 할당합니다. 일반적으로 유클리드 거리를 사용하지만 다른 측정항목도 사용할 수 있습니다. k-평균은 클러스터가 구형이고 크기가 동일하다고 가정하고 특성 간의 공분산을 무시합니다.

- 노트북 [kmeans_implementation](03_clustering_algorithms/02_kmeans_implementation.ipynb)은 k-Means 알고리즘의 작동 방식을 보여줍니다.

#### 결과 평가

클러스터 품질 메트릭은 대체 클러스터링 결과 중에서 선택하는 데 도움이 됩니다.

- [kmeans_evaluation](03_clustering_algorithms/03_kmeans_evaluation.ipynb) 노트북은 관성과 [실루엣 점수](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)을 사용하여 클러스터링 품질을 평가하는 방법을 보여줍니다.

### 코드 예: 계층적 클러스터링

계층적 군집화에서는 데이터가 점차 서로 다른 군집으로 연속적으로 병합될 수 있다고 가정하므로 대상 군집 수를 지정할 필요가 없습니다. 이는 글로벌 목표를 추구하지 않지만 단일 클러스터에서 개별 데이터 포인트로 구성된 클러스터에 이르기까지 일련의 중첩 클러스터를 생성하는 방법을 점진적으로 결정합니다.

계층적 클러스터링에는 k-평균과 같은 하이퍼파라미터가 없지만 클러스터 간의 차이점 측정(개별 데이터 포인트와 반대)은 클러스터링 결과에 중요한 영향을 미칩니다. 옵션은 다음과 같이 다릅니다.

- 단일 링크: 두 클러스터의 가장 가까운 이웃 간의 거리
- 완전한 링크: 각 클러스터 구성원 간의 최대 거리
- 그룹 평균
- Ward의 방법: 클러스터 내 분산 최소화

[계층적_클러스터링](03_clustering_algorithms/04_hierarchical_clustering.ipynb) 노트북은 이 알고리즘의 작동 방식과 결과를 시각화하고 평가하는 방법을 보여줍니다.

### 코드 예: 밀도 기반 클러스터링

밀도 기반 클러스터링 알고리즘은 다른 클러스터 구성원과의 근접성을 기반으로 클러스터 멤버십을 할당합니다. 그들은 임의의 모양과 크기의 밀집된 영역을 식별하는 목표를 추구합니다. 특정 개수의 클러스터를 지정할 필요는 없지만 대신 이웃 크기와 밀도 임계값을 정의하는 매개변수에 의존합니다.

[밀도_기반_클러스터링](03_clustering_algorithms/05_density_based_clustering.ipynb) 노트북은 DBSCAN 및 계층적 DBSCAN의 작동 방식을 보여줍니다.

- __자리 표시자__ 73 __

### 코드 예: 가우스 혼합 모델

GMM(가우스 혼합 모델)은 데이터가 다양한 다변량 정규 분포의 혼합에 의해 생성되었다고 가정하는 생성 모델입니다. 알고리즘은 이러한 분포의 평균 및 공분산 행렬을 추정하는 것을 목표로 합니다.

이는 k-평균 알고리즘을 일반화합니다. 즉, 클러스터가 구가 아닌 타원체가 될 수 있도록 특성 간에 공분산을 추가하고 중심은 각 분포의 평균으로 표시됩니다. GMM 알고리즘은 각 포인트가 모든 클러스터의 구성원이 될 확률을 갖기 때문에 소프트 할당을 수행합니다.

[gaussian_mixture_models](03_clustering_algorithms/06_gaussian_mixture_models.ipynb) 노트북은 GMM 클러스터링 모델의 적용을 보여줍니다.

### 코드 예: 계층적 위험 패리티

계층적 위험 패리티(HRP)의 핵심 아이디어는 공분산 행렬에서 계층적 클러스터링을 사용하여 유사한 상관 관계를 가진 자산을 그룹화하고 포트폴리오 구성 시 '유사한' 자산만 대체 자산으로 간주하여 자유도 수를 줄이는 것입니다. .

#### 알고리즘

하위 폴더 [hierarchical_risk_parity](04_hierarchical_risk_parity)에 있는 노트북 [hierarchical_risk_parity](04_hierarchical_risk_parity/01_hierarchical_risk_parity.ipynb)은 해당 응용 프로그램을 보여줍니다.

#### 대안과의 백테스트 비교

하위 폴더 [hierarchical_risk_parity](04_hierarchical_risk_parity)에 있는 노트북 [pf_optimization_with_hrp_zipline_benchmark](04_hierarchical_risk_parity/02_pf_optimization_with_hrp_zipline_benchmark.ipynb)은 HRP를 [제5장](../05_strategy_evaluation)에서 논의된 다른 포트폴리오 최적화 방법과 비교합니다.

### 참고자료

- [샘플 외 성과를 능가하는 다양한 포트폴리오 구축](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2708678), Lopez de Prado, 포트폴리오 관리 저널, 2015
- [계층적 클러스터링 기반 자산 배분](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2840729), 라피노 2016
- __자리표시자_82__



