# 랜덤 포레스트 - 일본 주식을 위한 롱숏 전략

이 장에서는 거래에 두 가지 새로운 종류의 기계 학습 모델인 의사결정 트리와 랜덤 포레스트를 사용하는 방법을 알아봅니다. 의사결정 트리가 입력 변수와 출력 변수 간의 비선형 관계를 인코딩하는 데이터로부터 규칙을 학습하는 방법을 살펴보겠습니다. 의사결정 트리를 훈련하고 이를 자산 수익률 및 가격 변동과 같은 회귀 및 분류 문제에 대한 예측에 사용하는 방법을 설명합니다. 또한 모델이 학습한 규칙을 시각화 및 해석하고 모델의 하이퍼파라미터를 조정하여 편향-분산 트레이드오프를 최적화하고 과적합을 방지합니다.

의사결정나무는 중요한 독립형 모델일 뿐만 아니라 다른 모델의 구성요소로도 자주 사용됩니다. 이 장의 두 번째 부분에서는 여러 개별 모델을 결합하여 예측 오류 분산이 낮은 단일 집계 예측을 생성하는 앙상블 모델을 소개합니다.

개별 모델의 구성을 무작위화하고 앙상블 구성 요소에 의해 발생하는 예측 오류의 상관 관계를 줄이는 여러 방법 중 하나로 흔히 배깅이라고 불리는 부트스트랩 집계를 설명합니다. 배깅이 분산을 효과적으로 줄이는 방법을 설명하고 랜덤 포레스트를 구성, 학습 및 조정하는 방법을 알아봅니다. 우리는 다수의 의사 결정 트리의 앙상블인 랜덤 포레스트가 해석의 일부 손실을 감수하면서 어떻게 예측 오류를 극적으로 줄일 수 있는지 살펴보겠습니다.

그런 다음 Random Forest 앙상블을 사용하여 지난 3년 동안 일본 대형주에 대한 수익성 있는 신호를 생성하는 장단기 거래 전략을 진행하고 구축할 것입니다. 우리는 주가 데이터를 소싱 및 준비하고, Random Forest 모델의 하이퍼파라미터를 조정하고, 모델의 신호를 기반으로 거래 규칙을 백테스트합니다. 그 결과 롱숏 전략은 시계열 모델에 대한 9장에서 본 공적분 관계 대신 기계 학습을 사용하여 주어진 투자 기간 동안 가격이 반대 방향으로 움직일 가능성이 있는 증권 바스켓을 식별하고 거래합니다.

## 콘텐츠

1. __자리표시자_0__
2. __자리표시자_1__
    * __자리표시자_2__
    * __자리표시자_3__
    * __자리표시자_4__
    * __자리표시자_5__
    * __자리표시자_6__
    * __자리표시자_7__
3. __자리표시자_8__
    * __자리표시자_9__
    * __자리표시자_10__
    * __자리표시자_11__
4. __자리표시자_12__
    * __자리표시자_13__
    * __자리표시자_14__
    * __자리표시자_15__
    * __자리표시자_16__
    * __자리표시자_17__

## 의사결정 트리: 데이터에서 규칙 학습

의사결정 트리는 데이터로부터 학습된 의사결정 규칙을 기반으로 목표 변수의 값을 예측하는 기계 학습 알고리즘입니다. 알고리즘이 규칙을 학습하는 방법을 제어하는 ​​목표를 변경하여 회귀 및 분류 문제에 알고리즘을 적용할 수 있습니다.

우리는 의사결정 트리가 규칙을 사용하여 예측하는 방법, (지속적인) 수익률과 가격 변동의 (범주형) 방향을 예측하도록 훈련하는 방법, 그리고 이를 효과적으로 해석, 시각화 및 조정하는 방법에 대해 논의할 것입니다.

## 코드 예: 실제 의사결정 트리

[결정_나무](01_decision_trees.ipynb) 노트북은 트리 기반 모델을 사용하여 통찰력을 얻고 예측하는 방법을 보여줍니다. 회귀 트리를 사용하는 방법을 보여주기 위해 수익을 예측하고 분류 사례에 대한 긍정적이거나 부정적인 자산 가격 변동을 예측합니다.

### 데이터: 월별 주식 수익률 및 특징

우리는 [제4장 알파팩터 연구](../04_alpha_factor_research)에 구성된 데이터 세트의 변형을 사용합니다. 2010~2017년 기간 동안 Quandl에서 제공하는 일일 주가와 다양한 엔지니어링 기능으로 구성됩니다. 
- 자세한 내용은 [data_prep](00_data_prep.ipynb) 노트에서 확인하실 수 있습니다.

### 시계열 데이터로 회귀 트리 구축

회귀 트리는 지정된 노드에 할당된 훈련 샘플의 평균 결과 값을 기반으로 예측을 수행하며 일반적으로 평균 제곱 오류를 사용하여 재귀 이진 분할 중에 최적의 규칙을 선택합니다.

### 분류 트리 구축

분류 트리는 결과의 범주형 특성으로 인해 예측 및 손실 측정에 다른 접근 방식이 필요하다는 점을 제외하면 회귀 버전과 동일하게 작동합니다. 회귀 트리는 연관된 훈련 표본의 평균 결과를 사용하여 리프 노드에 할당된 관측값에 대한 응답을 예측하는 반면 분류 트리는 해당 영역의 훈련 표본 중에서 가장 일반적인 클래스인 모드를 사용합니다. 분류 트리는 상대 클래스 빈도를 기반으로 확률적 예측을 생성할 수도 있습니다.

### 의사결정 트리 시각화

sklearn은 해당 라이브러리에서 사용하는 .dot 언어를 사용하여 트리에 대한 설명을 출력할 수 있으므로 [그래프로](https://graphviz.gitlab.io/download/) 라이브러리를 사용하여 트리를 시각화할 수 있습니다. 기능 및 클래스 레이블을 포함하도록 출력을 구성하고 다음과 같이 차트를 읽을 수 있도록 수준 수를 제한할 수 있습니다.

### 과적합 및 정규화

의사결정 트리는 특히 데이터세트에 샘플 수에 비해 특징 수가 많은 경우 과적합되는 경향이 높습니다. 노트북 [결정_나무](01_decision_trees.ipynb)에서는 관련 정규화 하이퍼 매개변수를 설명하고 그 사용법을 보여줍니다.

### 하이퍼파라미터를 조정하는 방법

또한 노트북은 하이퍼파라미터 조합에 대한 철저한 검색을 위해 `sklearn`의 [그리드검색CV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) 클래스를 포함한 교차 검증 사용을 보여줍니다.

## 랜덤 포레스트: 앙상블을 통한 더 나은 예측

의사결정 트리는 투명성과 해석 가능성 측면에서 유용할 뿐만 아니라 이전 섹션에서 설명한 과적합 및 높은 분산 문제를 해결하기 위해 설계를 무작위로 변경하는 전략과 많은 개별 트리를 결합하는 훨씬 더 강력한 앙상블 모델을 위한 기본 구성 요소이기도 합니다.

### 앙상블 모델이 더 나은 성능을 발휘하는 이유

앙상블 학습에는 개별 모델보다 더 나은 예측을 목표로 하는 여러 기계 학습 모델을 하나의 새로운 모델로 결합하는 작업이 포함됩니다. 보다 구체적으로 앙상블은 하나 이상의 주어진 학습 알고리즘을 사용하여 훈련된 여러 기본 추정기의 예측을 통합하여 이러한 모델이 자체적으로 생성할 수 있는 일반화 오류를 줄입니다.

### 코드 예: 배깅으로 모델 분산을 줄이는 방법

배깅은 대체 무작위 샘플인 부트스트랩 샘플의 집계를 나타냅니다. 이러한 무작위 표본에는 원래 데이터세트와 동일한 수의 관측치가 포함되지만 교체로 인해 중복된 관측치가 포함될 수 있습니다.

배깅은 예를 들어 각 트리가 어떻게 성장하는지 무작위화한 다음 일반화 오류를 줄이기 위해 예측을 평균화하여 기본 추정기의 분산을 줄입니다. 기본 알고리즘을 변경할 필요 없이 특정 모델을 개선하는 것이 간단한 접근 방식인 경우가 많습니다. 이는 과적합을 제한하는 것이 목표이기 때문에 심층 의사결정 트리와 같이 편향이 낮고 분산이 높은 복잡한 모델에서 가장 잘 작동합니다. 대조적으로 부스팅 방법은 얕은 의사결정 트리와 같은 약한 모델에서 가장 잘 작동합니다.

[bagged_decision_trees](02_bagged_decision_trees.ipynb) 노트북은 편향-분산 트레이드오프와 배깅이 개별 결정 트리와 비교하여 어떻게 분산을 줄이는지 보여줍니다.

### 코드 예: 랜덤 포레스트를 훈련하고 조정하는 방법

랜덤 포레스트 알고리즘은 배깅으로 생성된 부트스트랩 샘플에 의해 도입된 무작위화를 확장하여 분산을 더욱 줄이고 예측 성능을 향상시킵니다.

부트스트랩된 훈련 데이터에 대해 각 앙상블 멤버를 훈련하는 것 외에도 Random Forest는 모델에 사용된 기능에서 무작위로 샘플링합니다(교체 없음). 구현에 따라 각 트리 또는 각 분할에 대해 무작위 샘플을 추출할 수 있습니다. 결과적으로 알고리즘은 트리 수준이나 각 분할에 대해 새로운 규칙을 학습할 때 다양한 옵션에 직면하게 됩니다.

[random_forest_tuning](03_random_forest_tuning.ipynb) 노트북에는 이 섹션에 대한 구현 세부정보가 포함되어 있습니다.

## 코드 예: LightGBM을 사용한 일본 주식의 롱숏 신호

주로 그래디언트 부스팅을 위해 설계된 [제9장](../09, we used cointegration tests to identify pairs of stocks with a long-term equilibrium relationship in the form of a common trend to which their prices revert. 

In this chapter, we will use the predictions of a machine learning model to identify assets that are likely to go up or down so that we can enter market-neutral long and short positions accordingly. The approach is similar to our initial trading strategy that used linear regression in Chapter 7, Linear Models, and Chapter 8, Strategy Workflow: End-to-End Algo Trading.

Instead of the scikit-learn random forest implementation, we will use the [LightGBM](https://lightgbm.readthedocs.io/en/latest/) 패키지에 있습니다. 여러 장점 중 하나는 원-핫 더미 인코딩(Fisher 1958)을 사용하는 대신 범주형 변수를 숫자 기능으로 효율적으로 인코딩하는 LightGBM의 기능입니다. 다음 장에서 더 자세한 소개를 제공할 예정이지만, 코드 샘플은 scikit-learn 버전과 로직이 유사하므로 쉽게 따라할 수 있습니다.

### 맞춤형 집라인 번들

- [custom_bundle](00_custom_bundle) 디렉토리에는 데이터를 획득하고 맞춤형 Zipline 번들을 생성하는 방법에 대한 지침이 포함되어 있습니다.

### 기능 엔지니어링

- 노트북 [japanese_equity_features](04_japanese_equity_features.ipynb)은 모델 기능을 생성하는 방법을 보여줍니다.

### LightGBM 랜덤 포레스트 모델 튜닝

- 노트북 [random_forest_return_signals](05_random_forest_return_signals.ipynb)에는 [라이트GBM](https://lightgbm.readthedocs.io/en/latest/) 랜덤 포레스트 모델을 훈련하고 조정하는 코드가 포함되어 있습니다.

### Alphalens를 사용한 신호 평가

- [alphalens_signals_quality](06_alphalens_signals_quality.ipynb) 노트북은 [알파렌즈](https://github.com/quantopian/alphalens)을 사용하여 모델 예측을 평가하는 방법을 보여줍니다.

### Zipline을 이용한 백테스트

- 노트북 [백테스팅_with_zipline](07_backtesting_with_zipline.ipynb)은 [지퍼 라인](https://zipline.ml4trading.io/)을 사용하여 시뮬레이션된 롱-숏 전략을 사용하여 모델 예측을 평가합니다.

